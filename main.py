# --- ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨GPTè©•ä¾¡é–¢æ•° ---
def filter_grants_for_target_business(grants, location="é•·é‡çœŒå¡©å°»å¸‚", industry="æƒ…å ±é€šä¿¡æ¥­", employees=56):
    """å¯¾è±¡ä¼æ¥­ã«é©ã—ãŸåŠ©æˆé‡‘æƒ…å ±ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹"""
    # æƒ…å ±é€šä¿¡æ¥­å‘ã‘åŠ©æˆé‡‘ã«é–¢é€£ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    it_keywords = ['IT', 'ã‚·ã‚¹ãƒ†ãƒ ', 'ãƒ‡ã‚¸ã‚¿ãƒ«', 'æƒ…å ±é€šä¿¡', 'DX', 'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£', 'ã‚¢ãƒ—ãƒª', 'ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢', 'ICT', 'ã‚¯ãƒ©ã‚¦ãƒ‰', 'AI', 'IoT']
    
    # ä¸€æ¬¡ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆãƒ—ãƒ­ã‚°ãƒ©ãƒ çš„ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰
    filtered_grants = []
    
    for grant in grants:
        # åŸºæœ¬çš„ã«ã™ã¹ã¦ã®å…¨å›½å‘ã‘åŠ©æˆé‡‘ã¯å«ã‚ã‚‹
        include = True
        
        title = grant["title"].lower()
        desc = grant.get("description", "").lower()
        
        # ç‰¹å®šã®åœ°åŸŸé™å®šã§ã€ã‹ã¤å¯¾è±¡åœ°åŸŸã§ãªã„å ´åˆã¯é™¤å¤–
        if any(prefecture in title for prefecture in ['åŒ—æµ·é“', 'é’æ£®', 'å²©æ‰‹', 'å®®åŸ', 'ç§‹ç”°', 
                                               'å±±å½¢', 'ç¦å³¶', 'èŒ¨åŸ', 'æ ƒæœ¨', 'ç¾¤é¦¬', 
                                               'åŸ¼ç‰', 'åƒè‘‰', 'æ±äº¬', 'ç¥å¥ˆå·', 'æ–°æ½Ÿ', 
                                               'å¯Œå±±', 'çŸ³å·', 'ç¦äº•', 'å±±æ¢¨', 'å²é˜œ', 
                                               'é™å²¡', 'æ„›çŸ¥', 'ä¸‰é‡', 'æ»‹è³€', 'äº¬éƒ½', 
                                               'å¤§é˜ª', 'å…µåº«', 'å¥ˆè‰¯', 'å’Œæ­Œå±±', 'é³¥å–', 
                                               'å³¶æ ¹', 'å²¡å±±', 'åºƒå³¶', 'å±±å£', 'å¾³å³¶', 
                                               'é¦™å·', 'æ„›åª›', 'é«˜çŸ¥', 'ç¦å²¡', 'ä½è³€', 
                                               'é•·å´', 'ç†Šæœ¬', 'å¤§åˆ†', 'å®®å´', 'é¹¿å…å³¶', 
                                               'æ²–ç¸„']) and 'é•·é‡' not in title:
            include = False
        
        # ç‰¹å®šã®æ¥­ç¨®é™å®šã§ã€æƒ…å ±é€šä¿¡æ¥­ãŒå¯¾è±¡å¤–ã®å ´åˆã¯é™¤å¤–
        if (('è¾²æ¥­' in title or 'è¾²æ—' in title or 'æ¼æ¥­' in title) and 
            not any(kw.lower() in title.lower() or kw.lower() in desc.lower() for kw in it_keywords)):
            include = False
        
        if include:
            filtered_grants.append(grant)
    
    print(f"ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®åŠ©æˆé‡‘ä»¶æ•°: {len(filtered_grants)} ä»¶")
    return filtered_grants

def evaluate_grant_with_gpt(title, url, description, deadline, amount, ratio):
    """åŠ©æˆé‡‘æƒ…å ±ã‚’GPTã§è©•ä¾¡"""
    prompt = f"""
ã‚ãªãŸã¯ä¼æ¥­å‘ã‘åŠ©æˆé‡‘ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®åŠ©æˆé‡‘ãŒã€é•·é‡çœŒå¡©å°»å¸‚ã®æƒ…å ±é€šä¿¡æ¥­ãƒ»å¾“æ¥­å“¡56åã®ä¸­å°ä¼æ¥­ã«ã¨ã£ã¦ç”³è«‹å¯¾è±¡ã«ãªã‚‹ã‹ã€ã¾ãŸç”³è«‹å„ªå…ˆåº¦ï¼ˆé«˜ãƒ»ä¸­ãƒ»ä½ï¼‰ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

ã€åŠ©æˆé‡‘åã€‘{title}
ã€è©³ç´°URLã€‘{url}
ã€æ¦‚è¦ã€‘{description}
ã€ç”³è«‹æœŸé™ã€‘{deadline}
ã€åŠ©æˆé‡‘é¡ã€‘{amount}
ã€è£œåŠ©å‰²åˆã€‘{ratio}

å›ç­”å½¢å¼ã¯ä»¥ä¸‹ã§ãŠé¡˜ã„ã—ã¾ã™ï¼š
---
å¯¾è±¡ã‹ã©ã†ã‹: ï¼ˆã¯ã„ï¼ã„ã„ãˆï¼‰
ç†ç”±: ï¼ˆç°¡å˜ã«ï¼‰
ç”³è«‹å„ªå…ˆåº¦: ï¼ˆé«˜ï¼ä¸­ï¼ä½ï¼‰
---
"""
    try:
        response = openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"âŒ GPTè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {str(e)}"# --- è¿½åŠ æƒ…å ±ã‚½ãƒ¼ã‚¹é–¢æ•° ---
def scrape_additional_sources():
    """è¿½åŠ ã®æƒ…å ±ã‚½ãƒ¼ã‚¹ã‹ã‚‰åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ã™ã‚‹"""
    additional_grants = []
    
    # ãƒŸãƒ©ã‚µãƒplusï¼ˆä¸­å°ä¼æ¥­åºã®ç·åˆæ”¯æ´ã‚µã‚¤ãƒˆï¼‰ã‹ã‚‰æƒ…å ±å–å¾—
    try:
        print("ğŸ” ãƒŸãƒ©ã‚µãƒplusã®æƒ…å ±ã‚’å–å¾—ä¸­...")
        mirasapo_url = "https://mirasapo-plus.go.jp/subsidy/"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(mirasapo_url, headers=headers, timeout=30)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            
            # è£œåŠ©é‡‘ãƒ»åŠ©æˆé‡‘ã®ä¸€è¦§ã‚’å–å¾—
            subsidy_items = soup.select(".subsidy-item")
            
            for item in subsidy_items:
                title_elem = item.select_one(".subsidy-item-title")
                if not title_elem:
                    continue
                
                title = title_elem.text.strip()
                
                # URLã‚’å–å¾—
                link_elem = title_elem.find("a")
                if not link_elem or not link_elem.get("href"):
                    continue
                    
                url = link_elem.get("href")
                if not url.startswith("http"):
                    url = urljoin("https://mirasapo-plus.go.jp", url)
                
                # è©³ç´°æƒ…å ±ã‚’å–å¾—
                description = ""
                desc_elem = item.select_one(".subsidy-item-description")
                if desc_elem:
                    description = desc_elem.text.strip()
                
                # ç· ã‚åˆ‡ã‚Šã‚’å–å¾—
                deadline = "è¦ç¢ºèª"
                deadline_elem = item.select_one(".subsidy-item-deadline")
                if deadline_elem:
                    deadline = deadline_elem.text.strip()
                
                # åŠ©æˆé‡‘æƒ…å ±ã‚’è¿½åŠ 
                additional_grants.append({
                    "title": title,
                    "url": url,
                    "date": datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                    "description": description,
                    "deadline": deadline,
                    "amount": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª",
                    "ratio": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                })
            
            print(f"âœ… ãƒŸãƒ©ã‚µãƒplusã‹ã‚‰{len(additional_grants)}ä»¶ã®åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ ãƒŸãƒ©ã‚µãƒplusæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # çµŒæ¸ˆç”£æ¥­çœã®è£œåŠ©é‡‘ç·åˆã‚µã‚¤ãƒˆã‹ã‚‰æƒ…å ±å–å¾—
    try:
        print("ğŸ” çµŒæ¸ˆç”£æ¥­çœã®è£œåŠ©é‡‘æƒ…å ±ã‚’å–å¾—ä¸­...")
        meti_url = "https://www.meti.go.jp/policy/hojyokin/index.html"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(meti_url, headers=headers, timeout=30)
        if response.status_code == 200:
            response.encoding = 'utf-8'  # çµŒç”£çœã‚µã‚¤ãƒˆã¯æ–‡å­—ã‚³ãƒ¼ãƒ‰æŒ‡å®šãŒå¿…è¦ãªå ´åˆãŒã‚ã‚‹
            soup = BeautifulSoup(response.text, "html.parser")
            
            # è£œåŠ©é‡‘ãƒ»åŠ©æˆé‡‘ã®ä¸€è¦§ã‚’å–å¾—ï¼ˆçµŒç”£çœã‚µã‚¤ãƒˆã®æ§‹é€ ã«åˆã‚ã›ã¦èª¿æ•´ï¼‰
            subsidy_links = soup.select("a[href*='hojyo']") or soup.select("a[href*='subsidy']") or soup.select(".subsidy")
            previous_grants = len(additional_grants)
            
            for link in subsidy_links:
                title = link.text.strip()
                if not title or len(title) < 5:  # çŸ­ã™ãã‚‹ã‚¿ã‚¤ãƒˆãƒ«ã¯é™¤å¤–
                    continue
                
                url = link.get("href")
                if not url:
                    continue
                    
                if not url.startswith("http"):
                    url = urljoin("https://www.meti.go.jp", url)
                
                # åŠ©æˆé‡‘æƒ…å ±ã‚’è¿½åŠ 
                additional_grants.append({
                    "title": title,
                    "url": url,
                    "date": datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                    "description": "çµŒæ¸ˆç”£æ¥­çœã®åŠ©æˆé‡‘ãƒ»è£œåŠ©é‡‘åˆ¶åº¦",
                    "deadline": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª",
                    "amount": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª",
                    "ratio": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                })
            
            print(f"âœ… çµŒæ¸ˆç”£æ¥­çœã‹ã‚‰{len(additional_grants) - previous_grants}ä»¶ã®åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ çµŒæ¸ˆç”£æ¥­çœæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # Gãƒ“ã‚ºIDãƒãƒ¼ã‚¿ãƒ«ã‹ã‚‰åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—
    try:
        print("ğŸ” Gãƒ“ã‚ºIDãƒãƒ¼ã‚¿ãƒ«ã®æƒ…å ±ã‚’å–å¾—ä¸­...")
        gbiz_url = "https://gbiz-id.go.jp/subsidies/"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(gbiz_url, headers=headers, timeout=30)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            previous_grants = len(additional_grants)
            
            # è£œåŠ©é‡‘ãƒ»åŠ©æˆé‡‘ã®ä¸€è¦§ã‚’å–å¾—
            subsidy_items = soup.select(".subsidy-item") or soup.select(".subsidy-list li")
            
            for item in subsidy_items:
                title_elem = item.select_one(".subsidy-title") or item.select_one("h3") or item.select_one("strong")
                if not title_elem:
                    continue
                
                title = title_elem.text.strip()
                
                # URLã‚’å–å¾—
                link_elem = item.select_one("a")
                if not link_elem or not link_elem.get("href"):
                    continue
                    
                url = link_elem.get("href")
                if not url.startswith("http"):
                    url = urljoin("https://gbiz-id.go.jp", url)
                
                # è©³ç´°æƒ…å ±ã‚’å–å¾—
                description = ""
                desc_elem = item.select_one(".subsidy-description") or item.select_one("p")
                if desc_elem:
                    description = desc_elem.text.strip()
                
                # åŠ©æˆé‡‘æƒ…å ±ã‚’è¿½åŠ 
                additional_grants.append({
                    "title": title,
                    "url": url,
                    "date": datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                    "description": description,
                    "deadline": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª",
                    "amount": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª",
                    "ratio": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                })
            
            print(f"âœ… Gãƒ“ã‚ºIDãƒãƒ¼ã‚¿ãƒ«ã‹ã‚‰{len(additional_grants) - previous_grants}ä»¶ã®åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ Gãƒ“ã‚ºIDãƒãƒ¼ã‚¿ãƒ«æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # é•·é‡çœŒä¸­å°ä¼æ¥­æŒ¯èˆˆã‚»ãƒ³ã‚¿ãƒ¼ã®æƒ…å ±å–å¾—
    try:
        print("ğŸ” é•·é‡çœŒä¸­å°ä¼æ¥­æŒ¯èˆˆã‚»ãƒ³ã‚¿ãƒ¼ã®æƒ…å ±ã‚’å–å¾—ä¸­...")
        nagano_center_url = "https://www.nice-nagano.or.jp/topics/"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(nagano_center_url, headers=headers, timeout=30)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            previous_grants = len(additional_grants)
            
            # è£œåŠ©é‡‘ãƒ»åŠ©æˆé‡‘ã®ä¸€è¦§ã‚’å–å¾—ï¼ˆã‚µã‚¤ãƒˆæ§‹é€ ã«åˆã‚ã›ã¦èª¿æ•´ï¼‰
            subsidy_items = soup.select(".topics-list li") or soup.select(".news-list li")
            
            for item in subsidy_items:
                # è£œåŠ©é‡‘ãƒ»åŠ©æˆé‡‘ã«é–¢é€£ã™ã‚‹é …ç›®ã®ã¿ã‚’æŠ½å‡º
                item_text = item.text.lower()
                if not any(keyword in item_text for keyword in ['è£œåŠ©', 'åŠ©æˆ', 'æ”¯æ´é‡‘', 'çµ¦ä»˜é‡‘']):
                    continue
                
                title_elem = item.select_one("h3") or item.select_one("h4") or item.select_one("a") or item.select_one("strong")
                if not title_elem:
                    continue
                
                title = title_elem.text.strip()
                
                # URLã‚’å–å¾—
                link_elem = title_elem if title_elem.name == "a" else item.select_one("a")
                if not link_elem or not link_elem.get("href"):
                    continue
                    
                url = link_elem.get("href")
                if not url.startswith("http"):
                    url = urljoin("https://www.nice-nagano.or.jp", url)
                
                # è©³ç´°æƒ…å ±ã‚’å–å¾—
                description = item.text.strip()
                if title in description:
                    description = description.replace(title, "").strip()
                
                # æ—¥ä»˜ã‚’æŠ½å‡º
                date_elem = item.select_one(".date") or item.select_one("time")
                date_text = date_elem.text.strip() if date_elem else ""
                
                # ç· ã‚åˆ‡ã‚Šã‚’æŠ½å‡º
                deadline_match = re.search(r'([0-9]{4}å¹´[0-9]{1,2}æœˆ[0-9]{1,2}æ—¥).*(ç· åˆ‡|ç· ã‚åˆ‡ã‚Š|ã€†åˆ‡)', description)
                deadline = deadline_match.group(1) if deadline_match else "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                
                # åŠ©æˆé‡‘æƒ…å ±ã‚’è¿½åŠ 
                additional_grants.append({
                    "title": title,
                    "url": url,
                    "date": date_text if date_text else datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                    "description": description[:200] + "..." if len(description) > 200 else description,
                    "deadline": deadline,
                    "amount": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª",
                    "ratio": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                })
            
            print(f"âœ… é•·é‡çœŒä¸­å°ä¼æ¥­æŒ¯èˆˆã‚»ãƒ³ã‚¿ãƒ¼ã‹ã‚‰{len(additional_grants) - previous_grants}ä»¶ã®åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ é•·é‡çœŒä¸­å°ä¼æ¥­æŒ¯èˆˆã‚»ãƒ³ã‚¿ãƒ¼æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # æ—¥æœ¬å•†å·¥ä¼šè­°æ‰€ã®æƒ…å ±å–å¾—
    try:
        print("ğŸ” æ—¥æœ¬å•†å·¥ä¼šè­°æ‰€ã®åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ä¸­...")
        jcci_url = "https://www.jcci.or.jp/news/"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(jcci_url, headers=headers, timeout=30)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            previous_grants = len(additional_grants)
            
            # ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ã‹ã‚‰è£œåŠ©é‡‘ãƒ»åŠ©æˆé‡‘é–¢é€£ã®æƒ…å ±ã‚’å–å¾—
            news_items = soup.select(".news-list li") or soup.select(".news-item")
            
            for item in news_items:
                # è£œåŠ©é‡‘ãƒ»åŠ©æˆé‡‘ã«é–¢é€£ã™ã‚‹é …ç›®ã®ã¿ã‚’æŠ½å‡º
                item_text = item.text.lower()
                if not any(keyword in item_text for keyword in ['è£œåŠ©', 'åŠ©æˆ', 'æ”¯æ´é‡‘', 'çµ¦ä»˜é‡‘']):
                    continue
                
                # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
                title_elem = item.select_one("h3") or item.select_one("h4") or item.select_one("a")
                if not title_elem:
                    continue
                
                title = title_elem.text.strip()
                
                # URLã‚’å–å¾—
                link_elem = title_elem if title_elem.name == "a" else item.select_one("a")
                if not link_elem or not link_elem.get("href"):
                    continue
                    
                url = link_elem.get("href")
                if not url.startswith("http"):
                    url = urljoin("https://www.jcci.or.jp", url)
                
                # æ—¥ä»˜ã‚’å–å¾—
                date_elem = item.select_one(".date") or item.select_one("time")
                date_text = date_elem.text.strip() if date_elem else datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
                
                # åŠ©æˆé‡‘æƒ…å ±ã‚’è¿½åŠ 
                additional_grants.append({
                    "title": title,
                    "url": url,
                    "date": date_text,
                    "description": "æ—¥æœ¬å•†å·¥ä¼šè­°æ‰€ã‹ã‚‰ã®æƒ…å ±æä¾›",
                    "deadline": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª",
                    "amount": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª",
                    "ratio": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                })
            
            print(f"âœ… æ—¥æœ¬å•†å·¥ä¼šè­°æ‰€ã‹ã‚‰{len(additional_grants) - previous_grants}ä»¶ã®åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ æ—¥æœ¬å•†å·¥ä¼šè­°æ‰€æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # é‡è¤‡ã‚’æ’é™¤ã—ã¦è¿”ã™
    unique_grants = []
    urls = set()
    titles = set()
    
    for grant in additional_grants:
        # URLã¨ã‚¿ã‚¤ãƒˆãƒ«ã®ä¸¡æ–¹ãŒé‡è¤‡ã—ã¦ã„ãªã„å ´åˆã®ã¿è¿½åŠ 
        url_key = grant["url"].split("?")[0]  # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é™¤å¤–
        title_key = normalize_text(grant["title"])
        
        if url_key not in urls and title_key not in titles:
            urls.add(url_key)
            titles.add(title_key)
            unique_grants.append(grant)
    
    print(f"âœ… è¿½åŠ æƒ…å ±ã‚½ãƒ¼ã‚¹ã‹ã‚‰åˆè¨ˆ{len(unique_grants)}ä»¶ã®åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
    return unique_grants
                    "date": datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                    "description": description,
                    "deadline": deadline,
                    "amount": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª",
                    "ratio": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                })
            
            print(f"âœ… ãƒŸãƒ©ã‚µãƒplusã‹ã‚‰{len(additional_grants)}ä»¶ã®åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ ãƒŸãƒ©ã‚µãƒplusæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # çµŒæ¸ˆç”£æ¥­çœã®è£œåŠ©é‡‘ç·åˆã‚µã‚¤ãƒˆã‹ã‚‰æƒ…å ±å–å¾—
    try:
        print("ğŸ” çµŒæ¸ˆç”£æ¥­çœã®è£œåŠ©é‡‘æƒ…å ±ã‚’å–å¾—ä¸­...")
        meti_url = "https://www.meti.go.jp/policy/hojyokin/index.html"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(meti_url, headers=headers, timeout=30)
        if response.status_code == 200:
            response.encoding = 'utf-8'  # çµŒç”£çœã‚µã‚¤ãƒˆã¯æ–‡å­—ã‚³ãƒ¼ãƒ‰æŒ‡å®šãŒå¿…è¦ãªå ´åˆãŒã‚ã‚‹
            soup = BeautifulSoup(response.text, "html.parser")
            
            # è£œåŠ©é‡‘ãƒ»åŠ©æˆé‡‘ã®ä¸€è¦§ã‚’å–å¾—ï¼ˆçµŒç”£çœã‚µã‚¤ãƒˆã®æ§‹é€ ã«åˆã‚ã›ã¦èª¿æ•´ï¼‰
            subsidy_links = soup.select("a[href*='hojyo']") or soup.select("a[href*='subsidy']") or soup.select(".subsidy")
            previous_grants = len(additional_grants)
            
            for link in subsidy_links:
                title = link.text.strip()
                if not title or len(title) < 5:  # çŸ­ã™ãã‚‹ã‚¿ã‚¤ãƒˆãƒ«ã¯é™¤å¤–
                    continue
                
                url = link.get("href")
                if not url:
                    continue
                    
                if not url.startswith("http"):
                    url = urljoin("https://www.meti.go.jp", url)
                
                # åŠ©æˆé‡‘æƒ…å ±ã‚’è¿½åŠ 
                additional_grants.append({
                    "title": title,
                    "url": url,
                    "date": datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                    "description": "çµŒæ¸ˆç”£æ¥­çœã®åŠ©æˆé‡‘ãƒ»è£œåŠ©é‡‘åˆ¶åº¦",
                    "deadline": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª",
                    "amount": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª",
                    "ratio": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                })
            
            print(f"âœ… çµŒæ¸ˆç”£æ¥­çœã‹ã‚‰{len(additional_grants) - previous_grants}ä»¶ã®åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ çµŒæ¸ˆç”£æ¥­çœæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # Gãƒ“ã‚ºIDãƒãƒ¼ã‚¿ãƒ«ã‹ã‚‰åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—
    try:
        print("ğŸ” Gãƒ“ã‚ºIDãƒãƒ¼ã‚¿ãƒ«ã®æƒ…å ±ã‚’å–å¾—ä¸­...")
        gbiz_url = "https://gbiz-id.go.jp/subsidies/"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(gbiz_url, headers=headers, timeout=30)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            previous_grants = len(additional_grants)
            
            # è£œåŠ©é‡‘ãƒ»åŠ©æˆé‡‘ã®ä¸€è¦§ã‚’å–å¾—
            subsidy_items = soup.select(".subsidy-item") or soup.select(".subsidy-list li")
            
            for item in subsidy_items:
                title_elem = item.select_one(".subsidy-title") or item.select_one("h3") or item.select_one("strong")
                if not title_elem:
                    continue
                
                title = title_elem.text.strip()
                
                # URLã‚’å–å¾—
                link_elem = item.select_one("a")
                if not link_elem or not link_elem.get("href"):
                    continue
                    
                url = link_elem.get("href")
                if not url.startswith("http"):
                    url = urljoin("https://gbiz-id.go.jp", url)
                
                # è©³ç´°æƒ…å ±ã‚’å–å¾—
                description = ""
                desc_elem = item.select_one(".subsidy-description") or item.select_one("p")
                if desc_elem:
                    description = desc_elem.text.strip()
                
                # åŠ©æˆé‡‘æƒ…å ±ã‚’è¿½åŠ 
                additional_grants.append({
                    "title": title,
                    "url": url,
                    "date": datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                    "description": description,
                    "deadline": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª",
                    "amount": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª",
                    "ratio": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                })
            
            print(f"âœ… Gãƒ“ã‚ºIDãƒãƒ¼ã‚¿ãƒ«ã‹ã‚‰{len(additional_grants) - previous_grants}ä»¶ã®åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ Gãƒ“ã‚ºIDãƒãƒ¼ã‚¿ãƒ«æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # é•·é‡çœŒä¸­å°ä¼æ¥­æŒ¯èˆˆã‚»ãƒ³ã‚¿ãƒ¼ã®æƒ…å ±å–å¾—
    try:
        print("ğŸ” é•·é‡çœŒä¸­å°ä¼æ¥­æŒ¯èˆˆã‚»ãƒ³ã‚¿ãƒ¼ã®æƒ…å ±ã‚’å–å¾—ä¸­...")
        nagano_center_url = "https://www.nice-nagano.or.jp/topics/"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(nagano_center_url, headers=headers, timeout=30)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            previous_grants = len(additional_grants)
            
            # è£œåŠ©é‡‘ãƒ»åŠ©æˆé‡‘ã®ä¸€è¦§ã‚’å–å¾—ï¼ˆã‚µã‚¤ãƒˆæ§‹é€ ã«åˆã‚ã›ã¦èª¿æ•´ï¼‰
            subsidy_items = soup.select(".topics-list li") or soup.select(".news-list li")
            
            for item in subsidy_items:
                # è£œåŠ©é‡‘ãƒ»åŠ©æˆé‡‘ã«é–¢é€£ã™ã‚‹é …ç›®ã®ã¿ã‚’æŠ½å‡º
                item_text = item.text.lower()
                if not any(keyword in item_text for keyword in ['è£œåŠ©', 'åŠ©æˆ', 'æ”¯æ´é‡‘', 'çµ¦ä»˜é‡‘']):
                    continue
                
                title_elem = item.select_one("h3") or item.select_one("h4") or item.select_one("a") or item.select_one("strong")
                if not title_elem:
                    continue
                
                title = title_elem.text.strip()
                
                # URLã‚’å–å¾—
                link_elem = title_elem if title_elem.name == "a" else item.select_one("a")
                if not link_elem or not link_elem.get("href"):
                    continue
                    
                url = link_elem.get("href")
                if not url.startswith("http"):
                    url = urljoin("https://www.nice-nagano.or.jp", url)
                
                # è©³ç´°æƒ…å ±ã‚’å–å¾—
                description = item.text.strip()
                if title in description:
                    description = description.replace(title, "").strip()
                
                # æ—¥ä»˜ã‚’æŠ½å‡º
                date_elem = item.select_one(".date") or item.select_one("time")
                date_text = date_elem.text.strip() if date_elem else ""
                
                # ç· ã‚åˆ‡ã‚Šã‚’æŠ½å‡º
                deadline_match = re.search(r'([0-9]{4}å¹´[0-9]{1,2}æœˆ[0-9]{1,2}æ—¥).*(ç· åˆ‡|ç· ã‚åˆ‡ã‚Š|ã€†åˆ‡)', description)
                deadline = deadline_match.group(1) if deadline_match else "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                
                # åŠ©æˆé‡‘æƒ…å ±ã‚’è¿½åŠ 
                additional_grants.append({
                    "title": title,
                    "url": url,
                    "date": date_text if date_text else datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                    "description": description[:200] + "..." if len(description) > 200 else description,
                    "deadline": deadline,
                    "amount": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª",
                    "ratio": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                })
            
            print(f"âœ… é•·é‡çœŒä¸­å°ä¼æ¥­æŒ¯èˆˆã‚»ãƒ³ã‚¿ãƒ¼ã‹ã‚‰{len(additional_grants) - previous_grants}ä»¶ã®åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ é•·é‡çœŒä¸­å°ä¼æ¥­æŒ¯èˆˆã‚»ãƒ³ã‚¿ãƒ¼æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # æ—¥æœ¬å•†å·¥ä¼šè­°æ‰€ã®æƒ…å ±å–å¾—
    try:
        print("ğŸ” æ—¥æœ¬å•†å·¥ä¼šè­°æ‰€ã®åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ä¸­...")
        jcci_url = "https://www.jcci.or.jp/news/"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(jcci_url, headers=headers, timeout=30)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            previous_grants = len(additional_grants)
            
            # ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ã‹ã‚‰è£œåŠ©é‡‘ãƒ»åŠ©æˆé‡‘é–¢é€£ã®æƒ…å ±ã‚’å–å¾—
            news_items = soup.select(".news-list li") or soup.select(".news-item")
            
            for item in news_items:
                # è£œåŠ©é‡‘ãƒ»åŠ©æˆé‡‘ã«é–¢é€£ã™ã‚‹é …ç›®ã®ã¿ã‚’æŠ½å‡º
                item_text = item.text.lower()
                if not any(keyword in item_text for keyword in ['è£œåŠ©', 'åŠ©æˆ', 'æ”¯æ´é‡‘', 'çµ¦ä»˜é‡‘']):
                    continue
                
                # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
                title_elem = item.select_one("h3") or item.select_one("h4") or item.select_one("a")
                if not title_elem:
                    continue
                
                title = title_elem.text.strip()
                
                # URLã‚’å–å¾—
                link_elem = title_elem if title_elem.name == "a" else item.select_one("a")
                if not link_elem or not link_elem.get("href"):
                    continue
                    
                url = link_elem.get("href")
                if not url.startswith("http"):
                    url = urljoin("https://www.jcci.or.jp", url)
                
                # æ—¥ä»˜ã‚’å–å¾—
                date_elem = item.select_one(".date") or item.select_one("time")
                date_text = date_elem.text.strip() if date_elem else datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
                
                # åŠ©æˆé‡‘æƒ…å ±ã‚’è¿½åŠ 
                additional_grants.append({
                    "title": title,
                    "url": url,
                    "date": date_text,
                    "description": "æ—¥æœ¬å•†å·¥ä¼šè­°æ‰€ã‹ã‚‰ã®æƒ…å ±æä¾›",
                    "deadline": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª",
                    "amount": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª",
                    "ratio": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                })
            
            print(f"âœ… æ—¥æœ¬å•†å·¥ä¼šè­°æ‰€ã‹ã‚‰{len(additional_grants) - previous_grants}ä»¶ã®åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ æ—¥æœ¬å•†å·¥ä¼šè­°æ‰€æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # é‡è¤‡ã‚’æ’é™¤ã—ã¦è¿”ã™
    unique_grants = []
    urls = set()
    titles = set()
    
    for grant in additional_grants:
        # URLã¨ã‚¿ã‚¤ãƒˆãƒ«ã®ä¸¡æ–¹ãŒé‡è¤‡ã—ã¦ã„ãªã„å ´åˆã®ã¿è¿½åŠ 
        url_key = grant["url"].split("?")[0]  # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é™¤å¤–
        title_key = normalize_text(grant["title"])
        
        if url_key not in urls and title_key not in titles:
            urls.add(url_key)
            titles.add(title_key)
            unique_grants.append(grant)
    
    print(f"âœ… è¿½åŠ æƒ…å ±ã‚½ãƒ¼ã‚¹ã‹ã‚‰åˆè¨ˆ{len(unique_grants)}ä»¶ã®åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
    return unique_grants
                    "ratio": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                })
            
            print(f"âœ… ãƒŸãƒ©ã‚µãƒplusã‹ã‚‰{len(additional_grants)}ä»¶ã®åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ ãƒŸãƒ©ã‚µãƒplusæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # çµŒæ¸ˆç”£æ¥­çœã®è£œåŠ©é‡‘ç·åˆã‚µã‚¤ãƒˆã‹ã‚‰æƒ…å ±å–å¾—
    try:
        print("ğŸ” çµŒæ¸ˆç”£æ¥­çœã®è£œåŠ©é‡‘æƒ…å ±ã‚’å–å¾—ä¸­...")
        meti_url = "https://www.meti.go.jp/policy/hojyokin/index.html"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(meti_url, headers=headers, timeout=30)
        if response.status_code == 200:
            response.encoding = 'utf-8'  # çµŒç”£çœã‚µã‚¤ãƒˆã¯æ–‡å­—ã‚³ãƒ¼ãƒ‰æŒ‡å®šãŒå¿…è¦ãªå ´åˆãŒã‚ã‚‹
            soup = BeautifulSoup(response.text, "html.parser")
            
            # è£œåŠ©é‡‘ãƒ»åŠ©æˆé‡‘ã®ä¸€è¦§ã‚’å–å¾—ï¼ˆçµŒç”£çœã‚µã‚¤ãƒˆã®æ§‹é€ ã«åˆã‚ã›ã¦èª¿æ•´ï¼‰
            subsidy_links = soup.select("a[href*='hojyo']")
            
            for link in subsidy_links:
                title = link.text.strip()
                if not title or len(title) < 5:  # çŸ­ã™ãã‚‹ã‚¿ã‚¤ãƒˆãƒ«ã¯é™¤å¤–
                    continue
                
                url = link.get("href")
                if not url:
                    continue
                    
                if not url.startswith("http"):
                    url = urljoin("https://www.meti.go.jp", url)
                
                # åŠ©æˆé‡‘æƒ…å ±ã‚’è¿½åŠ 
                additional_grants.append({
                    "title": title,
                    "url": url,
                    "date": datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                    "description": "çµŒæ¸ˆç”£æ¥­çœã®åŠ©æˆé‡‘ãƒ»è£œåŠ©é‡‘åˆ¶åº¦",
                    "deadline": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª",
                    "amount": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª",
                    "ratio": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                })
            
            print(f"âœ… çµŒæ¸ˆç”£æ¥­çœã‹ã‚‰{len(additional_grants) - len(previous_grants)}ä»¶ã®åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
            previous_grants = len(additional_grants)
    except Exception as e:
        print(f"âŒ çµŒæ¸ˆç”£æ¥­çœæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # Gãƒ“ã‚ºIDãƒãƒ¼ã‚¿ãƒ«ã‹ã‚‰åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—
    try:
        print("ğŸ” Gãƒ“ã‚ºIDãƒãƒ¼ã‚¿ãƒ«ã®æƒ…å ±ã‚’å–å¾—ä¸­...")
        gbiz_url = "https://gbiz-id.go.jp/subsidies/"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(gbiz_url, headers=headers, timeout=30)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            
            # è£œåŠ©é‡‘ãƒ»åŠ©æˆé‡‘ã®ä¸€è¦§ã‚’å–å¾—
            subsidy_items = soup.select(".subsidy-item")
            
            for item in subsidy_items:
                title_elem = item.select_one(".subsidy-title")
                if not title_elem:
                    continue
                
                title = title_elem.text.strip()
                
                # URLã‚’å–å¾—
                link_elem = item.select_one("a")
                if not link_elem or not link_elem.get("href"):
                    continue
                    
                url = link_elem.get("href")
                if not url.startswith("http"):
                    url = urljoin("https://gbiz-id.go.jp", url)
                
                # è©³ç´°æƒ…å ±ã‚’å–å¾—
                description = ""
                desc_elem = item.select_one(".subsidy-description")
                if desc_elem:
                    description = desc_elem.text.strip()
                
                # åŠ©æˆé‡‘æƒ…å ±ã‚’è¿½åŠ 
                additional_grants.append({
                    "title": title,
                    "url": url,
                    "date": datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                    "description": description,
                    "deadline": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª",
                    "amount": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª",
                    "ratio": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                })
            
            print(f"âœ… Gãƒ“ã‚ºIDãƒãƒ¼ã‚¿ãƒ«ã‹ã‚‰{len(additional_grants) - len(previous_grants)}ä»¶ã®åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
            previous_grants = len(additional_grants)
    except Exception as e:
        print(f"âŒ Gãƒ“ã‚ºIDãƒãƒ¼ã‚¿ãƒ«æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # é•·é‡çœŒä¸­å°ä¼æ¥­æŒ¯èˆˆã‚»ãƒ³ã‚¿ãƒ¼ã®æƒ…å ±å–å¾—
    try:
        print("ğŸ” é•·é‡çœŒä¸­å°ä¼æ¥­æŒ¯èˆˆã‚»ãƒ³ã‚¿ãƒ¼ã®æƒ…å ±ã‚’å–å¾—ä¸­...")
        nagano_center_url = "https://www.nice-nagano.or.jp/subsidy/"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(nagano_center_url, headers=headers, timeout=30)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            
            # è£œåŠ©é‡‘ãƒ»åŠ©æˆé‡‘ã®ä¸€è¦§ã‚’å–å¾—ï¼ˆã‚µã‚¤ãƒˆæ§‹é€ ã«åˆã‚ã›ã¦èª¿æ•´ï¼‰
            subsidy_items = soup.select(".subsidy-list li") or soup.select(".subsidy-info")
            
            for item in subsidy_items:
                title_elem = item.select_one("h3") or item.select_one("h4") or item.select_one("strong")
                if not title_elem:
                    continue
                
                title = title_elem.text.strip()
                
                # URLã‚’å–å¾—
                link_elem = item.select_one("a")
                if not link_elem or not link_elem.get("href"):
                    continue
                    
                url = link_elem.get("href")
                if not url.startswith("http"):
                    url = urljoin("https://www.nice-nagano.or.jp", url)
                
                # è©³ç´°æƒ…å ±ã‚’å–å¾—
                description = item.text.strip()
                if title in description:
                    description = description.replace(title, "").strip()
                
                # ç· ã‚åˆ‡ã‚Šã‚’æŠ½å‡º
                deadline_match = re.search(r'([0-9]{4}å¹´[0-9]{1,2}æœˆ[0-9]{1,2}æ—¥).*(ç· åˆ‡|ç· ã‚åˆ‡ã‚Š|ã€†åˆ‡)', description)
                deadline = deadline_match.group(1) if deadline_match else "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                
                # åŠ©æˆé‡‘æƒ…å ±ã‚’è¿½åŠ 
                additional_grants.append({
                    "title": title,
                    "url": url,
                    "date": datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                    "description": description[:200] + "..." if len(description) > 200 else description,
                    "deadline": deadline,
                    "amount": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª",
                    "ratio": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                })
            
            print(f"âœ… é•·é‡çœŒä¸­å°ä¼æ¥­æŒ¯èˆˆã‚»ãƒ³ã‚¿ãƒ¼ã‹ã‚‰{len(additional_grants) - len(previous_grants)}ä»¶ã®åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
            previous_grants = len(additional_grants)
    except Exception as e:
        print(f"âŒ é•·é‡çœŒä¸­å°ä¼æ¥­æŒ¯èˆˆã‚»ãƒ³ã‚¿ãƒ¼æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # é‡è¤‡ã‚’æ’é™¤ã—ã¦è¿”ã™
    unique_grants = []
    urls = set()
    
    for grant in additional_grants:
        if grant["url"] not in urls:
            urls.add(grant["url"])
            unique_grants.append(grant)
    
    print(f"âœ… è¿½åŠ æƒ…å ±ã‚½ãƒ¼ã‚¹ã‹ã‚‰åˆè¨ˆ{len(unique_grants)}ä»¶ã®åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
    return unique_grants

# --- ãƒ¡ã‚¤ãƒ³é–¢æ•°ã®ä¿®æ­£ ---
def main():
    print("âœ… åŠ©æˆé‡‘æƒ…å ±å–å¾—é–‹å§‹")
    
    # J-Net21ã‹ã‚‰åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—
    grants = scrape_jnet21_grants()
    print(f"âœ… J-Net21ã‹ã‚‰åŠ©æˆé‡‘æƒ…å ±å–å¾—: {len(grants)} ä»¶")
    
    # è¿½åŠ æƒ…å ±ã‚½ãƒ¼ã‚¹ã‹ã‚‰åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—
    additional_grants = scrape_additional_sources()
    print(f"âœ… è¿½åŠ æƒ…å ±ã‚½ãƒ¼ã‚¹ã‹ã‚‰åŠ©æˆé‡‘æƒ…å ±å–å¾—: {len(additional_grants)} ä»¶")
    
    # åŠ©æˆé‡‘æƒ…å ±ã‚’ãƒãƒ¼ã‚¸
    grants.extend(additional_grants)
    
    # URLãƒ™ãƒ¼ã‚¹ã§é‡è¤‡ã‚’æ’é™¤
    unique_grants = []
    urls = set()
    
    for grant in grants:
        if grant["url"] not in urls:
            urls.add(grant["url"])
            unique_grants.append(grant)
    
    grants = unique_grants
    print(f"âœ… é‡è¤‡æ’é™¤å¾Œã®åŠ©æˆé‡‘ä»¶æ•°: {len(grants)} ä»¶")
    
    # å¯¾è±¡ä¼æ¥­å‘ã‘ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    grants = filter_grants_for_target_business(grants)
    
    # å–å¾—ã§ããŸä»¶æ•°ãŒå°‘ãªã„å ´åˆã¯ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
    if len(grants) < 3:
        print("âš ï¸ å–å¾—ã§ããŸåŠ©æˆé‡‘æƒ…å ±ãŒå°‘ãªã„ãŸã‚ã€ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨")
        backup_grants = get_national_grants()
        grants = backup_grants
    
    print(f"âœ… æœ€çµ‚åŠ©æˆé‡‘ä»¶æ•°: {len(grants)} ä»¶")

    # ä»¥ä¸‹ã¯æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã¨åŒã˜...
