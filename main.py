def normalize_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã®æ­£è¦åŒ–ã¨æ–‡å­—åŒ–ã‘é˜²æ­¢å‡¦ç†"""
    if not text:
        return ""
        
    # åˆ¶å¾¡æ–‡å­—ã‚„ç‰¹æ®Šæ–‡å­—ã®é™¤å»
    text = re.sub(r'[\x00-\x1F\x7F-\x9F]', '', text)
    
    # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’åŠè§’ã«å¤‰æ›
    text = text.replace('\u3000', ' ')
    
    # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«ã¾ã¨ã‚ã‚‹
    text = re.sub(r'\s+', ' ', text)
    
    # å‰å¾Œã®ç©ºç™½ã‚’å‰Šé™¤
    text = text.strip()
    
    return text#!/usr/bin/env python
# coding: utf-8

import os
import json
import requests
import openai
import gspread
import datetime
import re
from bs4 import BeautifulSoup
from google.oauth2 import service_account
from urllib.parse import urlparse, urljoin

# --- ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿ ---
SPREADSHEET_ID = os.getenv("SPREADSHEET_ID")
GOOGLE_SERVICE_ACCOUNT = os.getenv("GOOGLE_SERVICE_ACCOUNT")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
WEBHOOK_URL = os.getenv("WEBHOOK_URL")

# ç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯
if not WEBHOOK_URL:
    print("âŒ WEBHOOK_URL ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    exit(1)
else:
    # æœ€åˆã®æ•°æ–‡å­—ã ã‘ã‚’ãƒ­ã‚°ã«å‡ºã™ï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã®ãŸã‚ï¼‰
    webhook_preview = WEBHOOK_URL[:15] + "..." if len(WEBHOOK_URL) > 15 else WEBHOOK_URL
    print(f"âœ… WEBHOOK_URL: {webhook_preview}")

# --- OpenAIåˆæœŸåŒ– ---
openai.api_key = OPENAI_API_KEY

# --- Googleèªè¨¼ ---
try:
    credentials_info = json.loads(GOOGLE_SERVICE_ACCOUNT)
    credentials = service_account.Credentials.from_service_account_info(
        credentials_info,
        scopes=["https://www.googleapis.com/auth/spreadsheets", "https://www.googleapis.com/auth/drive"]
    )
    gc = gspread.authorize(credentials)
    sheet = gc.open_by_key(SPREADSHEET_ID).sheet1
    print("âœ… ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ¥ç¶šæˆåŠŸ")
except Exception as e:
    print(f"âŒ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ¥ç¶šå¤±æ•—: {e}")
    exit(1)

def scrape_jnet21_grants():
    """J-Net21ã‹ã‚‰é•·é‡çœŒã®è£œåŠ©é‡‘ãƒ»åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ã™ã‚‹"""
    base_url = "https://j-net21.smrj.go.jp/snavi/articles"
    params = {
        "category[]": 2,  # è£œåŠ©é‡‘ãƒ»åŠ©æˆé‡‘ãƒ»èè³‡ã‚«ãƒ†ã‚´ãƒª
        "order": "DESC",
        "perPage": 50,  # ã‚ˆã‚Šå¤šãã®çµæœã‚’å–å¾—
        "page": 1
    }
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    # é•·é‡çœŒé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    nagano_keywords = ['é•·é‡çœŒ', 'é•·é‡å¸‚', 'æ¾æœ¬å¸‚', 'ä¸Šç”°å¸‚', 'å²¡è°·å¸‚', 'é£¯ç”°å¸‚', 'è«è¨ªå¸‚', 'é ˆå‚å¸‚', 'å°è«¸å¸‚', 
                     'ä¼Šé‚£å¸‚', 'é§’ãƒ¶æ ¹å¸‚', 'ä¸­é‡å¸‚', 'å¤§ç”ºå¸‚', 'é£¯å±±å¸‚', 'èŒ…é‡å¸‚', 'å¡©å°»å¸‚', 'ä½ä¹…å¸‚', 'åƒæ›²å¸‚', 
                     'æ±å¾¡å¸‚', 'å®‰æ›‡é‡å¸‚', 'é•·é‡']
    
    print(f"ğŸ” J-Net21ã®è£œåŠ©é‡‘æƒ…å ±ã‚’æ¤œç´¢ä¸­...")
    try:
        response = requests.get(base_url, params=params, headers=headers, timeout=30)
        if response.status_code == 200:
            print(f"âœ… J-Net21ã‚µã‚¤ãƒˆã‚¢ã‚¯ã‚»ã‚¹æˆåŠŸ")
            response.encoding = response.apparent_encoding
            soup = BeautifulSoup(response.text, "html.parser")
            
            grants = []
            
            # è¨˜äº‹ã‚¢ã‚¤ãƒ†ãƒ ã‚’æ¢ã™
            article_items = soup.select(".m-panel-article")
            
            if article_items:
                print(f"âœ… è£œåŠ©é‡‘ãƒ»åŠ©æˆé‡‘è¨˜äº‹: {len(article_items)} ä»¶è¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
                
                for item in article_items:
                    # è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
                    title_elem = item.select_one(".m-panel-article__title")
                    
                    if title_elem and title_elem.text:
                        title = title_elem.text.strip()
                        
                        # é•·é‡çœŒé–¢é€£ã‹ã©ã†ã‹åˆ¤å®š
                        is_nagano_related = any(keyword in title for keyword in nagano_keywords)
                        is_national = not any(prefecture in title for prefecture in ['åŒ—æµ·é“', 'é’æ£®', 'å²©æ‰‹', 'å®®åŸ', 'ç§‹ç”°', 
                                                                                 'å±±å½¢', 'ç¦å³¶', 'èŒ¨åŸ', 'æ ƒæœ¨', 'ç¾¤é¦¬', 
                                                                                 'åŸ¼ç‰', 'åƒè‘‰', 'æ±äº¬', 'ç¥å¥ˆå·', 'æ–°æ½Ÿ', 
                                                                                 'å¯Œå±±', 'çŸ³å·', 'ç¦äº•', 'å±±æ¢¨', 'å²é˜œ', 
                                                                                 'é™å²¡', 'æ„›çŸ¥', 'ä¸‰é‡', 'æ»‹è³€', 'äº¬éƒ½', 
                                                                                 'å¤§é˜ª', 'å…µåº«', 'å¥ˆè‰¯', 'å’Œæ­Œå±±', 'é³¥å–', 
                                                                                 'å³¶æ ¹', 'å²¡å±±', 'åºƒå³¶', 'å±±å£', 'å¾³å³¶', 
                                                                                 'é¦™å·', 'æ„›åª›', 'é«˜çŸ¥', 'ç¦å²¡', 'ä½è³€', 
                                                                                 'é•·å´', 'ç†Šæœ¬', 'å¤§åˆ†', 'å®®å´', 'é¹¿å…å³¶', 
                                                                                 'æ²–ç¸„'])
                        
                        # å…¨å›½å¯¾è±¡ã¾ãŸã¯é•·é‡çœŒé–¢é€£ã®è£œåŠ©é‡‘ã®ã¿æŠ½å‡º
                        if is_nagano_related or is_national:
                            # æ—¥ä»˜è¦ç´ ã‚’å–å¾—
                            date_elem = item.select_one(".m-panel-article__date")
                            date_text = date_elem.text.strip() if date_elem else "æ—¥ä»˜ä¸æ˜"
                            
                            # ãƒªãƒ³ã‚¯ã‚’å–å¾—
                            link_elem = title_elem.find("a")
                            if link_elem and link_elem.get("href"):
                                link = link_elem.get("href")
                                
                                # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                                full_url = urljoin("https://j-net21.smrj.go.jp", link)
                                
                                # è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’å–å¾—
                                grant_details = scrape_grant_details(full_url)
                                
                                grant_info = {
                                    "title": title,
                                    "url": full_url,
                                    "date": date_text,
                                    "description": grant_details.get("description", "è©³ç´°ã¯è¦ç¢ºèª"),
                                    "deadline": grant_details.get("deadline", "è¦ç¢ºèª"),
                                    "amount": grant_details.get("amount", "è¦ç¢ºèª"),
                                    "ratio": grant_details.get("ratio", "è¦ç¢ºèª")
                                }
                                
                                grants.append(grant_info)
                                print(f"æŠ½å‡º: {title}")
            
            # å…¨å›½å‘ã‘ä¸€èˆ¬çš„ãªåŠ©æˆé‡‘æƒ…å ±ã‚‚è¿½åŠ 
            national_grants = get_national_grants()
            grants.extend(national_grants)
            
            return grants
        else:
            print(f"âŒ J-Net21ã‚µã‚¤ãƒˆã‚¢ã‚¯ã‚»ã‚¹å¤±æ•— (ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code})")
    except Exception as e:
        print(f"âŒ J-Net21ã‚µã‚¤ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ã‚¨ãƒ©ãƒ¼æ™‚ã«ã¯ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨ã—ã¦å›½ã®ä¸€èˆ¬çš„ãªåŠ©æˆé‡‘æƒ…å ±ã‚’è¿”ã™
    print("âš ï¸ J-Net21ã‹ã‚‰ã®å–å¾—ã«å¤±æ•—ã€‚ä¸€èˆ¬çš„ãªåŠ©æˆé‡‘æƒ…å ±ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    return get_national_grants()

def scrape_grant_details(url):
    """è£œåŠ©é‡‘ã®è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’å–å¾—ã™ã‚‹"""
    details = {
        "description": "",
        "deadline": "è¦ç¢ºèª",
        "amount": "è¦ç¢ºèª",
        "ratio": "è¦ç¢ºèª"
    }
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=30)
        if response.status_code == 200:
            response.encoding = response.apparent_encoding
            soup = BeautifulSoup(response.text, "html.parser")
            
            # è©³ç´°èª¬æ˜ã‚’å–å¾—
            content_elem = soup.select_one(".m-article__content")
            if content_elem:
                details["description"] = content_elem.text.strip()[:200] + "..."  # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚ã‚‹
            
            # ç· åˆ‡æ—¥ã‚’æ¢ã™
            deadline_patterns = [
                r'ç· åˆ‡.*?[ï¼š:]\s*(.*?[0-9]{4}å¹´[0-9]{1,2}æœˆ[0-9]{1,2}æ—¥)',
                r'ç”³è¾¼æœŸé™.*?[ï¼š:]\s*(.*?[0-9]{4}å¹´[0-9]{1,2}æœˆ[0-9]{1,2}æ—¥)', 
                r'å‹Ÿé›†æœŸé–“.*?[ï¼š:]\s*(.*?ã¾ã§)',
                r'å—ä»˜æœŸé–“.*?[ï¼š:]\s*(.*?ã¾ã§)',
                r'([0-9]{4}å¹´[0-9]{1,2}æœˆ[0-9]{1,2}æ—¥).*(ç· åˆ‡|ç· ã‚åˆ‡ã‚Š|ã€†åˆ‡)',
                r'([0-9]{4}å¹´[0-9]{1,2}æœˆ[0-9]{1,2}æ—¥.*?ã¾ã§)'
            ]
            
            text_content = response.text
            for pattern in deadline_patterns:
                match = re.search(pattern, text_content)
                if match:
                    details["deadline"] = match.group(1).strip()
                    break
            
            # è£œåŠ©é‡‘é¡ã‚’æ¢ã™
            amount_patterns = [
                r'è£œåŠ©é¡.*?[ï¼š:]\s*(.*?å††)',
                r'åŠ©æˆé¡.*?[ï¼š:]\s*(.*?å††)',
                r'è£œåŠ©é‡‘é¡.*?[ï¼š:]\s*(.*?å††)',
                r'ä¸Šé™.*?([0-9,]+ä¸‡å††)',
                r'ä¸Šé™é¡.*?([0-9,]+ä¸‡å††)',
                r'([0-9,]+ä¸‡å††).*?ä¸Šé™'
            ]
            
            for pattern in amount_patterns:
                match = re.search(pattern, text_content)
                if match:
                    details["amount"] = match.group(1).strip()
                    break
            
            # è£œåŠ©ç‡ã‚’æ¢ã™
            ratio_patterns = [
                r'è£œåŠ©ç‡.*?[ï¼š:]\s*(.*?åˆ†ã®.*?)',
                r'åŠ©æˆç‡.*?[ï¼š:]\s*(.*?åˆ†ã®.*?)',
                r'([0-9]/[0-9]ä»¥å†…)',
                r'([0-9]åˆ†ã®[0-9]ä»¥å†…)',
                r'è£œåŠ©ç‡.*(æœ€å¤§[0-9]{1,2}%)'
            ]
            
            for pattern in ratio_patterns:
                match = re.search(pattern, text_content)
                if match:
                    details["ratio"] = match.group(1).strip()
                    break
            
    except Exception as e:
        print(f"âŒ è©³ç´°ãƒšãƒ¼ã‚¸ã®å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    return details

def get_national_grants():
    """å…¨å›½å‘ã‘ã®ä¸»è¦åŠ©æˆé‡‘æƒ…å ±ã‚’Webã‚µã‚¤ãƒˆã‹ã‚‰å‹•çš„ã«å–å¾—ã™ã‚‹"""
    national_grants = []
    
    # ITå°å…¥è£œåŠ©é‡‘ã®æƒ…å ±ã‚’å–å¾—
    try:
        print("ğŸ” ITå°å…¥è£œåŠ©é‡‘ã®æƒ…å ±ã‚’å–å¾—ä¸­...")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # ITå°å…¥è£œåŠ©é‡‘2025ã®æƒ…å ±ã‚’å–å¾—
        it_hojo_url = "https://it-shien.smrj.go.jp/schedule/"
        response = requests.get(it_hojo_url, headers=headers, timeout=30)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            
            # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æƒ…å ±ã‚’å–å¾—
            schedule_tables = soup.select(".schedule-table")
            deadlines = []
            
            for table in schedule_tables:
                deadline_rows = table.select("tr")
                for row in deadline_rows:
                    if "ç· åˆ‡æ—¥" in row.text:
                        deadline_cells = row.select("td")
                        if deadline_cells:
                            deadlines.append(deadline_cells[0].text.strip())
            
            # ãƒ‹ãƒ¥ãƒ¼ã‚¹æƒ…å ±ã‹ã‚‰ã‚‚è©³ç´°ã‚’å–å¾—
            news_url = "https://it-shien.smrj.go.jp/news/20287"  # ITå°å…¥è£œåŠ©é‡‘2025æ¦‚è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹
            news_response = requests.get(news_url, headers=headers, timeout=30)
            
            if news_response.status_code == 200:
                news_soup = BeautifulSoup(news_response.text, "html.parser")
                news_content = news_soup.select_one(".m-article__content")
                
                if news_content:
                    content_text = news_content.text
                    
                    # é€šå¸¸æ ã®æƒ…å ±ã‚’æŠ½å‡º
                    normal_amount = re.search(r'é€šå¸¸æ .*?([0-9]+ä¸‡å††)', content_text)
                    normal_ratio = re.search(r'é€šå¸¸æ .*?è£œåŠ©ç‡.*?ã€Œ([^ã€]+)ã€', content_text)
                    
                    # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ ã®æƒ…å ±ã‚’æŠ½å‡º
                    security_amount = re.search(r'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–æ¨é€²æ .*?([0-9]+ä¸‡å††)', content_text)
                    security_ratio = re.search(r'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–æ¨é€²æ .*?è£œåŠ©ç‡.*?ã€Œ([^ã€]+)ã€', content_text)
                    
                    deadline = deadlines[0] if deadlines else "2025å¹´6æœˆé ƒï¼ˆè¦ç¢ºèªï¼‰"
                    
                    # é€šå¸¸æ ã®æƒ…å ±ã‚’è¿½åŠ 
                    national_grants.append({
                        "title": "ITå°å…¥è£œåŠ©é‡‘2025ï¼ˆé€šå¸¸æ ï¼‰", 
                        "url": "https://it-shien.smrj.go.jp/", 
                        "date": datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                        "description": "ä¸­å°ä¼æ¥­ãƒ»å°è¦æ¨¡äº‹æ¥­è€…å‘ã‘ã«ITãƒ„ãƒ¼ãƒ«å°å…¥ã‚’æ”¯æ´ã€‚æ¥­å‹™åŠ¹ç‡åŒ–ã‚„å£²ä¸Šå‘ä¸Šã«è²¢çŒ®ã™ã‚‹ITãƒ„ãƒ¼ãƒ«å°å…¥è²»ç”¨ã®ä¸€éƒ¨ã‚’è£œåŠ©ã€‚",
                        "deadline": deadline,
                        "amount": normal_amount.group(1) if normal_amount else "5ä¸‡å††ï½450ä¸‡å††",
                        "ratio": normal_ratio.group(1) if normal_ratio else "1/2ï¼ˆæœ€ä½è³ƒé‡‘è¿‘å‚ã®äº‹æ¥­è€…ã¯2/3ï¼‰"
                    })
                    
                    # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–æ¨é€²æ ã®æƒ…å ±ã‚’è¿½åŠ 
                    national_grants.append({
                        "title": "ITå°å…¥è£œåŠ©é‡‘2025ï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–æ¨é€²æ ï¼‰", 
                        "url": "https://it-shien.smrj.go.jp/security/", 
                        "date": datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                        "description": "ã‚µã‚¤ãƒãƒ¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–å¼·åŒ–ã‚’ç›®çš„ã¨ã—ãŸITãƒ„ãƒ¼ãƒ«å°å…¥ã‚’æ”¯æ´ã€‚",
                        "deadline": deadline,
                        "amount": security_amount.group(1) if security_amount else "5ä¸‡å††ï½150ä¸‡å††",
                        "ratio": security_ratio.group(1) if security_ratio else "1/2ï¼ˆå°è¦æ¨¡äº‹æ¥­è€…ã¯2/3ï¼‰"
                    })
        
        # äº‹æ¥­å†æ§‹ç¯‰è£œåŠ©é‡‘ã®æƒ…å ±ã‚’å–å¾—
        jigyou_saikouchiku_url = "https://jigyou-saikouchiku.go.jp/"
        response = requests.get(jigyou_saikouchiku_url, headers=headers, timeout=30)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            
            # ãƒ‹ãƒ¥ãƒ¼ã‚¹æƒ…å ±ã‹ã‚‰ãƒ‡ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³ã‚’å–å¾—
            news_items = soup.select(".news-list li")
            latest_news = ""
            
            for item in news_items:
                if "å…¬å‹Ÿ" in item.text and "é–‹å§‹" in item.text:
                    latest_news = item.text.strip()
                    break
            
            # äº‹æ¥­å†æ§‹ç¯‰è£œåŠ©é‡‘ã®æƒ…å ±ã‚’è¿½åŠ 
            national_grants.append({
                "title": "äº‹æ¥­å†æ§‹ç¯‰è£œåŠ©é‡‘ï¼ˆæœ€æ–°å…¬å‹Ÿï¼‰", 
                "url": "https://jigyou-saikouchiku.go.jp/", 
                "date": datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                "description": "ãƒã‚¹ãƒˆã‚³ãƒ­ãƒŠãƒ»ã‚¦ã‚£ã‚ºã‚³ãƒ­ãƒŠæ™‚ä»£ã®çµŒæ¸ˆç¤¾ä¼šå¤‰åŒ–ã«å¯¾å¿œã™ã‚‹ãŸã‚ã®æ–°åˆ†é‡å±•é–‹ã‚„æ¥­æ…‹è»¢æ›ç­‰ã‚’æ”¯æ´ã€‚",
                "deadline": latest_news if latest_news else "æœ€æ–°æƒ…å ±ã¯Webã‚µã‚¤ãƒˆã§è¦ç¢ºèª",
                "amount": "æœ€å¤§1å„„å††ï¼ˆæ ã«ã‚ˆã£ã¦ç•°ãªã‚‹ï¼‰",
                "ratio": "1/2ï½3/4ï¼ˆä¼æ¥­è¦æ¨¡ã‚„ç”³è«‹æ ã«ã‚ˆã£ã¦ç•°ãªã‚‹ï¼‰"
            })
        
        print(f"âœ… å…¨å›½å‘ã‘åŠ©æˆé‡‘æƒ…å ±å–å¾—å®Œäº†: {len(national_grants)}ä»¶")
        
    except Exception as e:
        print(f"âŒ å…¨å›½å‘ã‘åŠ©æˆé‡‘æƒ…å ±ã®å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # å–å¾—ã§ããŸå…¨å›½å‘ã‘åŠ©æˆé‡‘æƒ…å ±ãŒå°‘ãªã„å ´åˆã¯ã€æœ€ä½é™ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæƒ…å ±ã‚’è¿½åŠ 
    if len(national_grants) < 2:
        print("âš ï¸ å…¨å›½å‘ã‘åŠ©æˆé‡‘æƒ…å ±ã®å–å¾—ãŒä¸ååˆ†ãªãŸã‚ã€åŸºæœ¬æƒ…å ±ã‚’è£œå®Œã—ã¾ã™")
        
        # æœ€ä½é™ã®ITå°å…¥è£œåŠ©é‡‘æƒ…å ±
        if not any("ITå°å…¥è£œåŠ©é‡‘" in grant["title"] for grant in national_grants):
            national_grants.append({
                "title": "ITå°å…¥è£œåŠ©é‡‘2025ï¼ˆé€šå¸¸æ ï¼‰", 
                "url": "https://it-shien.smrj.go.jp/", 
                "date": datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                "description": "ä¸­å°ä¼æ¥­ãƒ»å°è¦æ¨¡äº‹æ¥­è€…å‘ã‘ã«ITãƒ„ãƒ¼ãƒ«å°å…¥ã‚’æ”¯æ´ã€‚æ¥­å‹™åŠ¹ç‡åŒ–ã‚„å£²ä¸Šå‘ä¸Šã«è²¢çŒ®ã™ã‚‹ITãƒ„ãƒ¼ãƒ«å°å…¥è²»ç”¨ã®ä¸€éƒ¨ã‚’è£œåŠ©ã€‚",
                "deadline": "2025å¹´6æœˆé ƒï¼ˆè©³ç´°ã¯Webã‚µã‚¤ãƒˆã§è¦ç¢ºèªï¼‰",
                "amount": "5ä¸‡å††ï½450ä¸‡å††",
                "ratio": "1/2ï¼ˆæœ€ä½è³ƒé‡‘è¿‘å‚ã®äº‹æ¥­è€…ã¯2/3ï¼‰"
            })
    
    # é•·é‡çœŒã®åŸºæœ¬åŠ©æˆé‡‘æƒ…å ±ã‚‚Webã‹ã‚‰å–å¾—
    try:
        print("ğŸ” é•·é‡çœŒã®åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ä¸­...")
        
        # é•·é‡çœŒã®Webã‚µã‚¤ãƒˆã‹ã‚‰æƒ…å ±å–å¾—
        nagano_urls = [
            "https://www.pref.nagano.lg.jp/keieishien/corona/kouzou-tenkan.html",  # é•·é‡çœŒãƒ—ãƒ©ã‚¹è£œåŠ©é‡‘
            "https://www.pref.nagano.lg.jp/rodokoyo/seisanseisupport.html"  # è³ƒä¸Šã’ãƒ»ç”Ÿç”£æ€§å‘ä¸Šã‚µãƒãƒ¼ãƒˆè£œåŠ©é‡‘
        ]
        
        for url in nagano_urls:
            try:
                response = requests.get(url, headers=headers, timeout=30)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, "html.parser")
                    
                    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
                    title_elem = soup.select_one("h1") or soup.select_one("h2")
                    title = title_elem.text.strip() if title_elem else "é•·é‡çœŒè£œåŠ©é‡‘"
                    
                    # å†…å®¹ã‚’å–å¾—
                    content_elem = soup.select_one("#main-contents") or soup.select_one("#tmp_contents")
                    content_text = content_elem.text if content_elem else ""
                    
                    # ç· ã‚åˆ‡ã‚Šã‚’æ¤œç´¢
                    deadline_patterns = [
                        r'ç· åˆ‡.*?[ï¼š:]\s*(.*?[0-9]{4}å¹´[0-9]{1,2}æœˆ[0-9]{1,2}æ—¥)',
                        r'ç”³è¾¼æœŸé™.*?[ï¼š:]\s*(.*?[0-9]{4}å¹´[0-9]{1,2}æœˆ[0-9]{1,2}æ—¥)', 
                        r'å‹Ÿé›†æœŸé–“.*?[ï¼š:]\s*(.*?ã¾ã§)',
                        r'å—ä»˜æœŸé–“.*?[ï¼š:]\s*(.*?ã¾ã§)',
                        r'([0-9]{4}å¹´[0-9]{1,2}æœˆ[0-9]{1,2}æ—¥).*(ç· åˆ‡|ç· ã‚åˆ‡ã‚Š|ã€†åˆ‡)'
                    ]
                    
                    deadline = "è¦ç¢ºèª"
                    for pattern in deadline_patterns:
                        match = re.search(pattern, content_text)
                        if match:
                            deadline = match.group(1).strip()
                            break
                    
                    # è£œåŠ©é‡‘é¡ã‚’æ¤œç´¢
                    amount_patterns = [
                        r'è£œåŠ©é¡.*?[ï¼š:]\s*(.*?å††)',
                        r'åŠ©æˆé¡.*?[ï¼š:]\s*(.*?å††)',
                        r'è£œåŠ©é‡‘é¡.*?[ï¼š:]\s*(.*?å††)',
                        r'ä¸Šé™.*?([0-9,]+ä¸‡å††)'
                    ]
                    
                    amount = "è¦ç¢ºèª"
                    for pattern in amount_patterns:
                        match = re.search(pattern, content_text)
                        if match:
                            amount = match.group(1).strip()
                            break
                    
                    # è£œåŠ©ç‡ã‚’æ¤œç´¢
                    ratio_patterns = [
                        r'è£œåŠ©ç‡.*?[ï¼š:]\s*(.*?åˆ†ã®.*?)',
                        r'åŠ©æˆç‡.*?[ï¼š:]\s*(.*?åˆ†ã®.*?)',
                        r'([0-9]/[0-9]ä»¥å†…)',
                        r'([0-9]åˆ†ã®[0-9]ä»¥å†…)'
                    ]
                    
                    ratio = "è¦ç¢ºèª"
                    for pattern in ratio_patterns:
                        match = re.search(pattern, content_text)
                        if match:
                            ratio = match.group(1).strip()
                            break
                    
                    # èª¬æ˜æ–‡ã‚’ç”Ÿæˆ
                    description = content_text[:200].replace("\n", " ").strip() + "..." if content_text else "é•·é‡çœŒã®è£œåŠ©é‡‘åˆ¶åº¦"
                    
                    # åŠ©æˆé‡‘æƒ…å ±ã‚’è¿½åŠ 
                    nagano_grant = {
                        "title": title,
                        "url": url,
                        "date": datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                        "description": description,
                        "deadline": deadline,
                        "amount": amount,
                        "ratio": ratio
                    }
                    
                    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ•´å½¢ï¼ˆé•·ã™ãã‚‹å ´åˆï¼‰
                    if len(nagano_grant["title"]) > 50:
                        if "ãƒ—ãƒ©ã‚¹è£œåŠ©é‡‘" in nagano_grant["title"]:
                            nagano_grant["title"] = "é•·é‡çœŒãƒ—ãƒ©ã‚¹è£œåŠ©é‡‘ï¼ˆä¸­å°ä¼æ¥­çµŒå–¶æ§‹é€ è»¢æ›ä¿ƒé€²äº‹æ¥­ï¼‰"
                        elif "è³ƒä¸Šã’" in nagano_grant["title"] or "ç”Ÿç”£æ€§å‘ä¸Š" in nagano_grant["title"]:
                            nagano_grant["title"] = "é•·é‡çœŒä¸­å°ä¼æ¥­è³ƒä¸Šã’ãƒ»ç”Ÿç”£æ€§å‘ä¸Šã‚µãƒãƒ¼ãƒˆè£œåŠ©é‡‘"
                        else:
                            nagano_grant["title"] = nagano_grant["title"][:50] + "..."
                    
                    national_grants.append(nagano_grant)
            
            except Exception as e:
                print(f"âŒ é•·é‡çœŒåŠ©æˆé‡‘æƒ…å ±ã®å–å¾—ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
        
        print(f"âœ… é•·é‡çœŒåŠ©æˆé‡‘æƒ…å ±å–å¾—å®Œäº†: {len(national_grants)}ä»¶")
        
    except Exception as e:
        print(f"âŒ é•·é‡çœŒåŠ©æˆé‡‘æƒ…å ±ã®å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    return national_grants

def filter_grants_for_target_business(grants, location="é•·é‡çœŒå¡©å°»å¸‚", industry="æƒ…å ±é€šä¿¡æ¥­", employees=56):
    """å¯¾è±¡ä¼æ¥­ã«é©ã—ãŸåŠ©æˆé‡‘æƒ…å ±ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹"""
    # æƒ…å ±é€šä¿¡æ¥­å‘ã‘åŠ©æˆé‡‘ã«é–¢é€£ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    it_keywords = ['IT', 'ã‚·ã‚¹ãƒ†ãƒ ', 'ãƒ‡ã‚¸ã‚¿ãƒ«', 'æƒ…å ±é€šä¿¡', 'DX', 'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£', 'ã‚¢ãƒ—ãƒª', 'ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢', 'ICT', 'ã‚¯ãƒ©ã‚¦ãƒ‰', 'AI', 'IoT']
    
    # æ¥­ç¨®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ã‚¯ã‚¨ãƒªã®æ§‹ç¯‰
    filter_query = f"""
ä»¥ä¸‹ã®åŠ©æˆé‡‘æƒ…å ±ãŒã€{location}ã®{industry}ï¼ˆå¾“æ¥­å“¡{employees}åã®ä¸­å°ä¼æ¥­ï¼‰ã«é©ç”¨å¯èƒ½ã‹ã©ã†ã‹ã‚’åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚
é©ç”¨å¯èƒ½ãªå ´åˆã¯ã€Œã¯ã„ã€ã€ãã†ã§ãªã„å ´åˆã¯ã€Œã„ã„ãˆã€ã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚

{json.dumps(grants, ensure_ascii=False, indent=2)}

å›ç­”å½¢å¼: ã¯ã„/ã„ã„ãˆ
"""
    
    # ä¸€æ¬¡ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆãƒ—ãƒ­ã‚°ãƒ©ãƒ çš„ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰
    filtered_grants = []
    
    for grant in grants:
        # åŸºæœ¬çš„ã«ã™ã¹ã¦ã®å…¨å›½å‘ã‘åŠ©æˆé‡‘ã¯å«ã‚ã‚‹
        include = True
        
        title = grant["title"].lower()
        desc = grant["description"].lower() if "description" in grant else ""
        
        # ç‰¹å®šã®åœ°åŸŸé™å®šã§ã€ã‹ã¤å¯¾è±¡åœ°åŸŸã§ãªã„å ´åˆã¯é™¤å¤–
        if any(prefecture in title for prefecture in ['åŒ—æµ·é“', 'é’æ£®', 'å²©æ‰‹', 'å®®åŸ', 'ç§‹ç”°', 
                                                 'å±±å½¢', 'ç¦å³¶', 'èŒ¨åŸ', 'æ ƒæœ¨', 'ç¾¤é¦¬', 
                                                 'åŸ¼ç‰', 'åƒè‘‰', 'æ±äº¬', 'ç¥å¥ˆå·', 'æ–°æ½Ÿ', 
                                                 'å¯Œå±±', 'çŸ³å·', 'ç¦äº•', 'å±±æ¢¨', 'å²é˜œ', 
                                                 'é™å²¡', 'æ„›çŸ¥', 'ä¸‰é‡', 'æ»‹è³€', 'äº¬éƒ½', 
                                                 'å¤§é˜ª', 'å…µåº«', 'å¥ˆè‰¯', 'å’Œæ­Œå±±', 'é³¥å–', 
                                                 'å³¶æ ¹', 'å²¡å±±', 'åºƒå³¶', 'å±±å£', 'å¾³å³¶', 
                                                 'é¦™å·', 'æ„›åª›', 'é«˜çŸ¥', 'ç¦å²¡', 'ä½è³€', 
                                                 'é•·å´', 'ç†Šæœ¬', 'å¤§åˆ†', 'å®®å´', 'é¹¿å…å³¶', 
                                                 'æ²–ç¸„']) and 'é•·é‡' not in title:
            include = False
        
        # ç‰¹å®šã®æ¥­ç¨®é™å®šã§ã€æƒ…å ±é€šä¿¡æ¥­ãŒå¯¾è±¡å¤–ã®å ´åˆã¯é™¤å¤–
        if (('è¾²æ¥­' in title or 'è¾²æ—' in title or 'æ¼æ¥­' in title) and 
            not any(kw.lower() in title.lower() or kw.lower() in desc.lower() for kw in it_keywords)):
            include = False
        
        if include:
            filtered_grants.append(grant)
    
    print(f"ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®åŠ©æˆé‡‘ä»¶æ•°: {len(filtered_grants)} ä»¶")
    return filtered_grants

def evaluate_grant_with_gpt(title, url, description, deadline, amount, ratio):
    """åŠ©æˆé‡‘æƒ…å ±ã‚’GPTã§è©•ä¾¡"""
    prompt = f"""
ã‚ãªãŸã¯ä¼æ¥­å‘ã‘åŠ©æˆé‡‘ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®åŠ©æˆé‡‘ãŒã€é•·é‡çœŒå¡©å°»å¸‚ã®æƒ…å ±é€šä¿¡æ¥­ãƒ»å¾“æ¥­å“¡56åã®ä¸­å°ä¼æ¥­ã«ã¨ã£ã¦ç”³è«‹å¯¾è±¡ã«ãªã‚‹ã‹ã€ã¾ãŸç”³è«‹å„ªå…ˆåº¦ï¼ˆé«˜ãƒ»ä¸­ãƒ»ä½ï¼‰ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

ã€åŠ©æˆé‡‘åã€‘{title}
ã€è©³ç´°URLã€‘{url}
ã€æ¦‚è¦ã€‘{description}
ã€ç”³è«‹æœŸé™ã€‘{deadline}
ã€åŠ©æˆé‡‘é¡ã€‘{amount}
ã€è£œåŠ©å‰²åˆã€‘{ratio}

å›ç­”å½¢å¼ã¯ä»¥ä¸‹ã§ãŠé¡˜ã„ã—ã¾ã™ï¼š
---
å¯¾è±¡ã‹ã©ã†ã‹: ï¼ˆã¯ã„ï¼ã„ã„ãˆï¼‰
ç†ç”±: ï¼ˆç°¡å˜ã«ï¼‰
ç”³è«‹å„ªå…ˆåº¦: ï¼ˆé«˜ï¼ä¸­ï¼ä½ï¼‰
---
"""
    try:
        response = openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"âŒ GPTè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {str(e)}"

def send_to_google_chat(message, webhook_url):
    """Google Chatã«é€šçŸ¥ã‚’é€ä¿¡"""
    if not message.strip():
        print("âŒ é€ä¿¡ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒç©ºã§ã™")
        message = "åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚·ã‚¹ãƒ†ãƒ ç®¡ç†è€…ã«ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
    
    # URLã®æ¤œè¨¼
    if not webhook_url or not webhook_url.startswith("https://"):
        print("âŒ ç„¡åŠ¹ãªwebhook URLã§ã™")
        return
    
    headers = {"Content-Type": "application/json; charset=UTF-8"}
    
    current_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M')
    
    # æ–‡å­—åŒ–ã‘é˜²æ­¢ã®ãŸã‚ã®å‰å‡¦ç†
    # ç‰¹æ®Šæ–‡å­—ã‚„åˆ¶å¾¡æ–‡å­—ã‚’é™¤å»
    clean_message = re.sub(r'[\x00-\x1F\x7F-\x9F]', '', message)
    
    # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ†ã‚­ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼
    payload = {"text": f"ğŸ“¢ åŠ©æˆé‡‘æ”¯æ´åˆ¶åº¦è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ\næ›´æ–°æ—¥æ™‚: {current_time}\n\n{clean_message}"}
    
    try:
        print(f"â³ Google Chatã«é€ä¿¡ä¸­... ({webhook_url[:15]}...)")
        encoded_payload = json.dumps(payload, ensure_ascii=False).encode('utf-8')
        response = requests.post(
            webhook_url, 
            headers=headers, 
            data=encoded_payload
        )
        print(f"å¿œç­”ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
        print(f"å¿œç­”æœ¬æ–‡: {response.text[:100]}")  # æœ€åˆã®100æ–‡å­—ã ã‘è¡¨ç¤º
        
        if response.status_code == 200:
            print(f"âœ… Google Chatã«é€šçŸ¥ã—ã¾ã—ãŸã€‚ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
        else:
            print(f"âŒ Google Chaté€ä¿¡ã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}")
            print(f"å¿œç­”æœ¬æ–‡: {response.text}")
    except Exception as e:
        print(f"âŒ Google Chaté€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆå†…å®¹: {encoded_payload[:200].decode('utf-8')}...")  # æœ€åˆã®200æ–‡å­—ã ã‘è¡¨ç¤º

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
def main():
    print("âœ… åŠ©æˆé‡‘æƒ…å ±å–å¾—é–‹å§‹")
    
    # J-Net21ã‹ã‚‰åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—
    grants = scrape_jnet21_grants()
    print(f"âœ… åŠ©æˆé‡‘æƒ…å ±å–å¾—: {len(grants)} ä»¶")
    
    # å¯¾è±¡ä¼æ¥­å‘ã‘ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    grants = filter_grants_for_target_business(grants)
    
    # å–å¾—ã§ããŸä»¶æ•°ãŒå°‘ãªã„å ´åˆã¯ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
    if len(grants) < 3:
        print("âš ï¸ å–å¾—ã§ããŸåŠ©æˆé‡‘æƒ…å ±ãŒå°‘ãªã„ãŸã‚ã€ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨")
        backup_grants = get_national_grants()
        grants = backup_grants
    
    print(f"âœ… æœ€çµ‚åŠ©æˆé‡‘ä»¶æ•°: {len(grants)} ä»¶")

    # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆåˆæœŸåŒ–
    try:
        sheet.clear()
        headers = ["No.", "ã‚¿ã‚¤ãƒˆãƒ«", "URL", "ç”³è«‹æœŸé™", "åŠ©æˆé‡‘é¡", "è£œåŠ©å‰²åˆ", "å¯¾è±¡ã‹ã©ã†ã‹", "ç†ç”±", "ç”³è«‹å„ªå…ˆåº¦"]
        sheet.append_row(headers)
        print("âœ… ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆåˆæœŸåŒ–å®Œäº†")
    except Exception as e:
        print(f"âŒ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ“ä½œã‚¨ãƒ©ãƒ¼: {e}")
        # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡ã—ã¦çµ‚äº†
        send_to_google_chat("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®æ“ä½œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚", WEBHOOK_URL)
        return

    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å†…å®¹ã‚’åˆæœŸåŒ–
    full_message = ""

    for i, grant in enumerate(grants, start=1):
        title = normalize_text(grant["title"])
        url = grant["url"]
        description = normalize_text(grant.get("description", ""))
        deadline = normalize_text(grant.get("deadline", "è¦ç¢ºèª"))
        amount = normalize_text(grant.get("amount", "è¦ç¢ºèª"))
        ratio = normalize_text(grant.get("ratio", "è¦ç¢ºèª"))

        print(f"â³ {i}ä»¶ç›® è©•ä¾¡ä¸­...")
        result = evaluate_grant_with_gpt(title, url, description, deadline, amount, ratio)
        print(f"âœ… {i}ä»¶ç›® è©•ä¾¡å®Œäº†")

        # GPTå›ç­”ã®åˆ†è§£ï¼ˆæ­£è¦è¡¨ç¾ã‚’ä½¿ã£ã¦å …ç‰¢ã«ï¼‰
        target = re.search(r"å¯¾è±¡ã‹ã©ã†ã‹:?\s*(.+)", result)
        target = normalize_text(target.group(1).strip() if target else "ä¸æ˜")
        
        reason = re.search(r"ç†ç”±:?\s*(.+)", result)
        reason = normalize_text(reason.group(1).strip() if reason else "ä¸æ˜")
        
        priority = re.search(r"ç”³è«‹å„ªå…ˆåº¦:?\s*(.+)", result)
        priority = normalize_text(priority.group(1).strip() if priority else "ä¸æ˜")

        try:
            sheet.append_row([i, title, url, deadline, amount, ratio, target, reason, priority])
            print(f"âœ… {i}ä»¶ç›® ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ›¸ãè¾¼ã¿å®Œäº†")
        except Exception as e:
            print(f"âŒ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

        # å„åŠ©æˆé‡‘æƒ…å ±ã‚’ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«è¿½åŠ 
        full_message += f"*{i}. {title}*\n"
        full_message += f"ãƒ»å¯¾è±¡: *{target}*\n"
        full_message += f"ãƒ»å„ªå…ˆåº¦: *{priority}*\n"
        full_message += f"ãƒ»ç”³è«‹æœŸé™: {deadline}\n"
        full_message += f"ãƒ»åŠ©æˆé‡‘é¡: {amount}\n"
        full_message += f"ãƒ»è£œåŠ©å‰²åˆ: {ratio}\n"
        full_message += f"ãƒ»ç†ç”±: {reason}\n"
        full_message += f"ãƒ»URL: {url}\n\n"

    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒç©ºã§ãªã„ã“ã¨ã‚’ç¢ºèªã—ã¦ã‹ã‚‰é€ä¿¡
    if full_message:
        send_to_google_chat(full_message, WEBHOOK_URL)
    else:
        print("âŒ é€ä¿¡ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚ã‚Šã¾ã›ã‚“")
        send_to_google_chat("åŠ©æˆé‡‘æƒ…å ±ã®è©•ä¾¡çµæœã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", WEBHOOK_URL)

if __name__ == "__main__":
    main()
