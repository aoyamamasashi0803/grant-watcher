#!/usr/bin/env python
# coding: utf-8

import os
import json
import requests
import openai
import gspread
import datetime
import re
from bs4 import BeautifulSoup
from google.oauth2 import service_account
from urllib.parse import urlparse, urljoin

# --- ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿ ---
SPREADSHEET_ID = os.getenv("SPREADSHEET_ID")
GOOGLE_SERVICE_ACCOUNT = os.getenv("GOOGLE_SERVICE_ACCOUNT")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
WEBHOOK_URL = os.getenv("WEBHOOK_URL")

# ç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯
if not WEBHOOK_URL:
    print("âŒ WEBHOOK_URL ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    exit(1)
else:
    # æœ€åˆã®æ•°æ–‡å­—ã ã‘ã‚’ãƒ­ã‚°ã«å‡ºã™ï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã®ãŸã‚ï¼‰
    webhook_preview = WEBHOOK_URL[:15] + "..." if len(WEBHOOK_URL) > 15 else WEBHOOK_URL
    print(f"âœ… WEBHOOK_URL: {webhook_preview}")

# --- OpenAIåˆæœŸåŒ– ---
openai.api_key = OPENAI_API_KEY

# --- Googleèªè¨¼ ---
try:
    credentials_info = json.loads(GOOGLE_SERVICE_ACCOUNT)
    credentials = service_account.Credentials.from_service_account_info(
        credentials_info,
        scopes=["https://www.googleapis.com/auth/spreadsheets", "https://www.googleapis.com/auth/drive"]
    )
    gc = gspread.authorize(credentials)
    sheet = gc.open_by_key(SPREADSHEET_ID).sheet1
    print("âœ… ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ¥ç¶šæˆåŠŸ")
except Exception as e:
    print(f"âŒ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ¥ç¶šå¤±æ•—: {e}")
    exit(1)

# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---
def normalize_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã®æ­£è¦åŒ–ã¨æ–‡å­—åŒ–ã‘é˜²æ­¢å‡¦ç†"""
    if not text:
        return ""
        
    # åˆ¶å¾¡æ–‡å­—ã‚„ç‰¹æ®Šæ–‡å­—ã®é™¤å»
    text = re.sub(r'[\x00-\x1F\x7F-\x9F]', '', text)
    
    # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’åŠè§’ã«å¤‰æ›
    text = text.replace('\u3000', ' ')
    
    # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«ã¾ã¨ã‚ã‚‹
    text = re.sub(r'\s+', ' ', text)
    
    # å‰å¾Œã®ç©ºç™½ã‚’å‰Šé™¤
    text = text.strip()
    
    return text

def generate_simple_title(original_title, index):
    """ã‚¿ã‚¤ãƒˆãƒ«ãŒæ–‡å­—åŒ–ã‘ã—ã¦ã„ã‚‹ã‹ä¸é©åˆ‡ãªå ´åˆã«ã€ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆã™ã‚‹"""
    # æ–‡å­—åŒ–ã‘ã‚„ä¸é©åˆ‡ãªæ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    if re.search(r'[^\w\s\d\u3000-\u9FFF\u3040-\u309F\u30A0-\u30FF,.?!:;\-()[\]{}ã€Œã€ã€ã€ï¼ˆï¼‰ï¼»ï¼½ï½›ï½ã€ã€‚ï¼Ÿï¼ï¼šï¼›]', original_title):
        # ä¸»è¦ãªã‚«ãƒ†ã‚´ãƒªã«åŸºã¥ã„ã¦ã‚¿ã‚¤ãƒˆãƒ«ã‚’å‰²ã‚Šå½“ã¦
        if "IT" in original_title or "å°å…¥" in original_title:
            return f"ITå°å…¥è£œåŠ©é‡‘ (#{index})"
        elif "äº‹æ¥­å†æ§‹ç¯‰" in original_title or "å†æ§‹ç¯‰" in original_title:
            return f"äº‹æ¥­å†æ§‹ç¯‰è£œåŠ©é‡‘ (#{index})"
        elif "é•·é‡çœŒ" in original_title and "ãƒ—ãƒ©ã‚¹" in original_title:
            return f"é•·é‡çœŒãƒ—ãƒ©ã‚¹è£œåŠ©é‡‘ (#{index})"
        elif "é•·é‡çœŒ" in original_title and ("è³ƒä¸Šã’" in original_title or "ç”Ÿç”£æ€§" in original_title):
            return f"é•·é‡çœŒä¸­å°ä¼æ¥­è³ƒä¸Šã’ãƒ»ç”Ÿç”£æ€§å‘ä¸Šã‚µãƒãƒ¼ãƒˆè£œåŠ©é‡‘ (#{index})"
        elif "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£" in original_title:
            return f"ITå°å…¥è£œåŠ©é‡‘(ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–æ¨é€²æ ) (#{index})"
        else:
            return f"åŠ©æˆé‡‘æƒ…å ± (#{index})"
    else:
        # å•é¡Œãªã‘ã‚Œã°ãã®ã¾ã¾è¿”ã™
        return original_title

# --- ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•° ---
def scrape_jnet21_grants():
    """J-Net21ã‹ã‚‰é•·é‡çœŒã®è£œåŠ©é‡‘ãƒ»åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ã™ã‚‹"""
    base_url = "https://j-net21.smrj.go.jp/snavi/articles"
    params = {
        "category[]": 2,  # è£œåŠ©é‡‘ãƒ»åŠ©æˆé‡‘ãƒ»èè³‡ã‚«ãƒ†ã‚´ãƒª
        "order": "DESC",
        "perPage": 50,  # ã‚ˆã‚Šå¤šãã®çµæœã‚’å–å¾—
        "page": 1
    }
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    # é•·é‡çœŒé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    nagano_keywords = ['é•·é‡çœŒ', 'é•·é‡å¸‚', 'æ¾æœ¬å¸‚', 'ä¸Šç”°å¸‚', 'å²¡è°·å¸‚', 'é£¯ç”°å¸‚', 'è«è¨ªå¸‚', 'é ˆå‚å¸‚', 'å°è«¸å¸‚', 
                     'ä¼Šé‚£å¸‚', 'é§’ãƒ¶æ ¹å¸‚', 'ä¸­é‡å¸‚', 'å¤§ç”ºå¸‚', 'é£¯å±±å¸‚', 'èŒ…é‡å¸‚', 'å¡©å°»å¸‚', 'ä½ä¹…å¸‚', 'åƒæ›²å¸‚', 
                     'æ±å¾¡å¸‚', 'å®‰æ›‡é‡å¸‚', 'é•·é‡']
    
    print(f"ğŸ” J-Net21ã®è£œåŠ©é‡‘æƒ…å ±ã‚’æ¤œç´¢ä¸­...")
    try:
        response = requests.get(base_url, params=params, headers=headers, timeout=30)
        if response.status_code == 200:
            print(f"âœ… J-Net21ã‚µã‚¤ãƒˆã‚¢ã‚¯ã‚»ã‚¹æˆåŠŸ")
            response.encoding = response.apparent_encoding
            soup = BeautifulSoup(response.text, "html.parser")
            
            grants = []
            
            # è¨˜äº‹ã‚¢ã‚¤ãƒ†ãƒ ã‚’æ¢ã™
            article_items = soup.select(".m-panel-article")
            
            if article_items:
                print(f"âœ… è£œåŠ©é‡‘ãƒ»åŠ©æˆé‡‘è¨˜äº‹: {len(article_items)} ä»¶è¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
                
                for item in article_items:
                    # è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
                    title_elem = item.select_one(".m-panel-article__title")
                    
                    if title_elem and title_elem.text:
                        title = title_elem.text.strip()
                        
                        # é•·é‡çœŒé–¢é€£ã‹ã©ã†ã‹åˆ¤å®š
                        is_nagano_related = any(keyword in title for keyword in nagano_keywords)
                        is_national = not any(prefecture in title for prefecture in ['åŒ—æµ·é“', 'é’æ£®', 'å²©æ‰‹', 'å®®åŸ', 'ç§‹ç”°', 
                                                                                 'å±±å½¢', 'ç¦å³¶', 'èŒ¨åŸ', 'æ ƒæœ¨', 'ç¾¤é¦¬', 
                                                                                 'åŸ¼ç‰', 'åƒè‘‰', 'æ±äº¬', 'ç¥å¥ˆå·', 'æ–°æ½Ÿ', 
                                                                                 'å¯Œå±±', 'çŸ³å·', 'ç¦äº•', 'å±±æ¢¨', 'å²é˜œ', 
                                                                                 'é™å²¡', 'æ„›çŸ¥', 'ä¸‰é‡', 'æ»‹è³€', 'äº¬éƒ½', 
                                                                                 'å¤§é˜ª', 'å…µåº«', 'å¥ˆè‰¯', 'å’Œæ­Œå±±', 'é³¥å–', 
                                                                                 'å³¶æ ¹', 'å²¡å±±', 'åºƒå³¶', 'å±±å£', 'å¾³å³¶', 
                                                                                 'é¦™å·', 'æ„›åª›', 'é«˜çŸ¥', 'ç¦å²¡', 'ä½è³€', 
                                                                                 'é•·å´', 'ç†Šæœ¬', 'å¤§åˆ†', 'å®®å´', 'é¹¿å…å³¶', 
                                                                                 'æ²–ç¸„']) and 'é•·é‡' not in title
                        
                        # å…¨å›½å¯¾è±¡ã¾ãŸã¯é•·é‡çœŒé–¢é€£ã®è£œåŠ©é‡‘ã®ã¿æŠ½å‡º
                        if is_nagano_related or is_national:
                            # æ—¥ä»˜è¦ç´ ã‚’å–å¾—
                            date_elem = item.select_one(".m-panel-article__date")
                            date_text = date_elem.text.strip() if date_elem else "æ—¥ä»˜ä¸æ˜"
                            
                            # ãƒªãƒ³ã‚¯ã‚’å–å¾—
                            link_elem = title_elem.find("a")
                            if link_elem and link_elem.get("href"):
                                link = link_elem.get("href")
                                
                                # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                                full_url = urljoin("https://j-net21.smrj.go.jp", link)
                                
                                # è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’å–å¾—
                                grant_details = scrape_grant_details(full_url)
                                
                                grant_info = {
                                    "title": title,
                                    "url": full_url,
                                    "date": date_text,
                                    "description": grant_details.get("description", "è©³ç´°ã¯è¦ç¢ºèª"),
                                    "deadline": grant_details.get("deadline", "è¦ç¢ºèª"),
                                    "amount": grant_details.get("amount", "è¦ç¢ºèª"),
                                    "ratio": grant_details.get("ratio", "è¦ç¢ºèª")
                                }
                                
                                grants.append(grant_info)
                                print(f"æŠ½å‡º: {title}")
            
            # å…¨å›½å‘ã‘ä¸€èˆ¬çš„ãªåŠ©æˆé‡‘æƒ…å ±ã‚‚è¿½åŠ 
            national_grants = get_national_grants()
            grants.extend(national_grants)
            
            return grants
        else:
            print(f"âŒ J-Net21ã‚µã‚¤ãƒˆã‚¢ã‚¯ã‚»ã‚¹å¤±æ•— (ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code})")
    except Exception as e:
        print(f"âŒ J-Net21ã‚µã‚¤ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ã‚¨ãƒ©ãƒ¼æ™‚ã«ã¯ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨ã—ã¦å›½ã®ä¸€èˆ¬çš„ãªåŠ©æˆé‡‘æƒ…å ±ã‚’è¿”ã™
    print("âš ï¸ J-Net21ã‹ã‚‰ã®å–å¾—ã«å¤±æ•—ã€‚ä¸€èˆ¬çš„ãªåŠ©æˆé‡‘æƒ…å ±ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    return get_national_grants()

def scrape_grant_details(url):
    """è£œåŠ©é‡‘ã®è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’å–å¾—ã™ã‚‹"""
    details = {
        "description": "",
        "deadline": "è¦ç¢ºèª",
        "amount": "è¦ç¢ºèª",
        "ratio": "è¦ç¢ºèª"
    }
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=30)
        if response.status_code == 200:
            response.encoding = response.apparent_encoding
            soup = BeautifulSoup(response.text, "html.parser")
            
            # è©³ç´°èª¬æ˜ã‚’å–å¾—
            content_elem = soup.select_one(".m-article__content")
            if content_elem:
                details["description"] = content_elem.text.strip()[:200] + "..."  # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚ã‚‹
            
            # ç· åˆ‡æ—¥ã‚’æ¢ã™
            deadline_patterns = [
                r'ç· åˆ‡.*?[ï¼š:]\s*(.*?[0-9]{4}å¹´[0-9]{1,2}æœˆ[0-9]{1,2}æ—¥)',
                r'ç”³è¾¼æœŸé™.*?[ï¼š:]\s*(.*?[0-9]{4}å¹´[0-9]{1,2}æœˆ[0-9]{1,2}æ—¥)', 
                r'å‹Ÿé›†æœŸé–“.*?[ï¼š:]\s*(.*?ã¾ã§)',
                r'å—ä»˜æœŸé–“.*?[ï¼š:]\s*(.*?ã¾ã§)',
                r'([0-9]{4}å¹´[0-9]{1,2}æœˆ[0-9]{1,2}æ—¥).*(ç· åˆ‡|ç· ã‚åˆ‡ã‚Š|ã€†åˆ‡)',
                r'([0-9]{4}å¹´[0-9]{1,2}æœˆ[0-9]{1,2}æ—¥.*?ã¾ã§)'
            ]
            
            text_content = response.text
            for pattern in deadline_patterns:
                match = re.search(pattern, text_content)
                if match:
                    details["deadline"] = match.group(1).strip()
                    break
            
            # è£œåŠ©é‡‘é¡ã‚’æ¢ã™
            amount_patterns = [
                r'è£œåŠ©é¡.*?[ï¼š:]\s*(.*?å††)',
                r'åŠ©æˆé¡.*?[ï¼š:]\s*(.*?å††)',
                r'è£œåŠ©é‡‘é¡.*?[ï¼š:]\s*(.*?å††)',
                r'ä¸Šé™.*?([0-9,]+ä¸‡å††)',
                r'ä¸Šé™é¡.*?([0-9,]+ä¸‡å††)',
                r'([0-9,]+ä¸‡å††).*?ä¸Šé™'
            ]
            
            for pattern in amount_patterns:
                match = re.search(pattern, text_content)
                if match:
                    details["amount"] = match.group(1).strip()
                    break
            
            # è£œåŠ©ç‡ã‚’æ¢ã™
            ratio_patterns = [
                r'è£œåŠ©ç‡.*?[ï¼š:]\s*(.*?åˆ†ã®.*?)',
                r'åŠ©æˆç‡.*?[ï¼š:]\s*(.*?åˆ†ã®.*?)',
                r'([0-9]/[0-9]ä»¥å†…)',
                r'([0-9]åˆ†ã®[0-9]ä»¥å†…)',
                r'è£œåŠ©ç‡.*(æœ€å¤§[0-9]{1,2}%)'
            ]
            
            for pattern in ratio_patterns:
                match = re.search(pattern, text_content)
                if match:
                    details["ratio"] = match.group(1).strip()
                    break
            
    except Exception as e:
        print(f"âŒ è©³ç´°ãƒšãƒ¼ã‚¸ã®å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    return details

# --- å…¨å›½å‘ã‘åŠ©æˆé‡‘æƒ…å ±å–å¾—é–¢æ•° ---
def get_national_grants():
    """å…¨å›½å‘ã‘ã®ä¸»è¦åŠ©æˆé‡‘æƒ…å ±ã‚’Webã‚µã‚¤ãƒˆã‹ã‚‰å‹•çš„ã«å–å¾—ã™ã‚‹"""
    national_grants = []
    
    # ITå°å…¥è£œåŠ©é‡‘ã®æƒ…å ±ã‚’å–å¾—
    try:
        print("ğŸ” ITå°å…¥è£œåŠ©é‡‘ã®æƒ…å ±ã‚’å–å¾—ä¸­...")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # ITå°å…¥è£œåŠ©é‡‘2025ã®æƒ…å ±ã‚’å–å¾—
        it_hojo_url = "https://it-shien.smrj.go.jp/schedule/"
        response = requests.get(it_hojo_url, headers=headers, timeout=30)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            
            # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æƒ…å ±ã‚’å–å¾—
            schedule_tables = soup.select(".schedule-table")
            deadlines = []
            
            for table in schedule_tables:
                deadline_rows = table.select("tr")
                for row in deadline_rows:
                    if "ç· åˆ‡æ—¥" in row.text:
                        deadline_cells = row.select("td")
                        if deadline_cells:
                            deadlines.append(deadline_cells[0].text.strip())
            
            # ãƒ‹ãƒ¥ãƒ¼ã‚¹æƒ…å ±ã‹ã‚‰ã‚‚è©³ç´°ã‚’å–å¾—
            news_url = "https://it-shien.smrj.go.jp/news/20287"  # ITå°å…¥è£œåŠ©é‡‘2025æ¦‚è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹
            news_response = requests.get(news_url, headers=headers, timeout=30)
            
            if news_response.status_code == 200:
                news_soup = BeautifulSoup(news_response.text, "html.parser")
                news_content = news_soup.select_one(".m-article__content")
                
                if news_content:
                    content_text = news_content.text
                    
                    # é€šå¸¸æ ã®æƒ…å ±ã‚’æŠ½å‡º
                    normal_amount = re.search(r'é€šå¸¸æ .*?([0-9]+ä¸‡å††)', content_text)
                    normal_ratio = re.search(r'é€šå¸¸æ .*?è£œåŠ©ç‡.*?ã€Œ([^ã€]+)ã€', content_text)
                    
                    # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ ã®æƒ…å ±ã‚’æŠ½å‡º
                    security_amount = re.search(r'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–æ¨é€²æ .*?([0-9]+ä¸‡å††)', content_text)
                    security_ratio = re.search(r'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–æ¨é€²æ .*?è£œåŠ©ç‡.*?ã€Œ([^ã€]+)ã€', content_text)
                    
                    deadline = deadlines[0] if deadlines else "2025å¹´6æœˆé ƒï¼ˆè¦ç¢ºèªï¼‰"
                    
                    # é€šå¸¸æ ã®æƒ…å ±ã‚’è¿½åŠ 
                    national_grants.append({
                        "title": "ITå°å…¥è£œåŠ©é‡‘2025ï¼ˆé€šå¸¸æ ï¼‰", 
                        "url": "https://it-shien.smrj.go.jp/", 
                        "date": datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                        "description": "ä¸­å°ä¼æ¥­ãƒ»å°è¦æ¨¡äº‹æ¥­è€…å‘ã‘ã«ITãƒ„ãƒ¼ãƒ«å°å…¥ã‚’æ”¯æ´ã€‚æ¥­å‹™åŠ¹ç‡åŒ–ã‚„å£²ä¸Šå‘ä¸Šã«è²¢çŒ®ã™ã‚‹ITãƒ„ãƒ¼ãƒ«å°å…¥è²»ç”¨ã®ä¸€éƒ¨ã‚’è£œåŠ©ã€‚",
                        "deadline": deadline,
                        "amount": normal_amount.group(1) if normal_amount else "5ä¸‡å††ï½450ä¸‡å††",
                        "ratio": normal_ratio.group(1) if normal_ratio else "1/2ï¼ˆæœ€ä½è³ƒé‡‘è¿‘å‚ã®äº‹æ¥­è€…ã¯2/3ï¼‰"
                    })
                    
                    # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–æ¨é€²æ ã®æƒ…å ±ã‚’è¿½åŠ 
                    national_grants.append({
                        "title": "ITå°å…¥è£œåŠ©é‡‘2025ï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–æ¨é€²æ ï¼‰", 
                        "url": "https://it-shien.smrj.go.jp/security/", 
                        "date": datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                        "description": "ã‚µã‚¤ãƒãƒ¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–å¼·åŒ–ã‚’ç›®çš„ã¨ã—ãŸITãƒ„ãƒ¼ãƒ«å°å…¥ã‚’æ”¯æ´ã€‚",
                        "deadline": deadline,
                        "amount": security_amount.group(1) if security_amount else "5ä¸‡å††ï½150ä¸‡å††",
                        "ratio": security_ratio.group(1) if security_ratio else "1/2ï¼ˆå°è¦æ¨¡äº‹æ¥­è€…ã¯2/3ï¼‰"
                    })
        
        # äº‹æ¥­å†æ§‹ç¯‰è£œåŠ©é‡‘ã®æƒ…å ±ã‚’å–å¾—
        jigyou_saikouchiku_url = "https://jigyou-saikouchiku.go.jp/"
        response = requests.get(jigyou_saikouchiku_url, headers=headers, timeout=30)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            
            # ãƒ‹ãƒ¥ãƒ¼ã‚¹æƒ…å ±ã‹ã‚‰ãƒ‡ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³ã‚’å–å¾—
            news_items = soup.select(".news-list li")
            latest_news = ""
            
            for item in news_items:
                if "å…¬å‹Ÿ" in item.text and "é–‹å§‹" in item.text:
                    latest_news = item.text.strip()
                    break
            
            # äº‹æ¥­å†æ§‹ç¯‰è£œåŠ©é‡‘ã®æƒ…å ±ã‚’è¿½åŠ 
            national_grants.append({
                "title": "äº‹æ¥­å†æ§‹ç¯‰è£œåŠ©é‡‘ï¼ˆæœ€æ–°å…¬å‹Ÿï¼‰", 
                "url": "https://jigyou-saikouchiku.go.jp/", 
                "date": datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                "description": "ãƒã‚¹ãƒˆã‚³ãƒ­ãƒŠãƒ»ã‚¦ã‚£ã‚ºã‚³ãƒ­ãƒŠæ™‚ä»£ã®çµŒæ¸ˆç¤¾ä¼šå¤‰åŒ–ã«å¯¾å¿œã™ã‚‹ãŸã‚ã®æ–°åˆ†é‡å±•é–‹ã‚„æ¥­æ…‹è»¢æ›ç­‰ã‚’æ”¯æ´ã€‚",
                "deadline": latest_news if latest_news else "æœ€æ–°æƒ…å ±ã¯Webã‚µã‚¤ãƒˆã§è¦ç¢ºèª",
                "amount": "æœ€å¤§1å„„å††ï¼ˆæ ã«ã‚ˆã£ã¦ç•°ãªã‚‹ï¼‰",
                "ratio": "1/2ï½3/4ï¼ˆä¼æ¥­è¦æ¨¡ã‚„ç”³è«‹æ ã«ã‚ˆã£ã¦ç•°ãªã‚‹ï¼‰"
            })
        
        print(f"âœ… å…¨å›½å‘ã‘åŠ©æˆé‡‘æƒ…å ±å–å¾—å®Œäº†: {len(national_grants)}ä»¶")
        
    except Exception as e:
        print(f"âŒ å…¨å›½å‘ã‘åŠ©æˆé‡‘æƒ…å ±ã®å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # å–å¾—ã§ããŸå…¨å›½å‘ã‘åŠ©æˆé‡‘æƒ…å ±ãŒå°‘ãªã„å ´åˆã¯ã€æœ€ä½é™ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæƒ…å ±ã‚’è¿½åŠ 
    if len(national_grants) < 2:
        print("âš ï¸ å…¨å›½å‘ã‘åŠ©æˆé‡‘æƒ…å ±ã®å–å¾—ãŒä¸ååˆ†ãªãŸã‚ã€åŸºæœ¬æƒ…å ±ã‚’è£œå®Œã—ã¾ã™")
        
        # æœ€ä½é™ã®ITå°å…¥è£œåŠ©é‡‘æƒ…å ±
        if not any("ITå°å…¥è£œåŠ©é‡‘" in grant["title"] for grant in national_grants):
            national_grants.append({
                "title": "ITå°å…¥è£œåŠ©é‡‘2025ï¼ˆé€šå¸¸æ ï¼‰", 
                "url": "https://it-shien.smrj.go.jp/", 
                "date": datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                "description": "ä¸­å°ä¼æ¥­ãƒ»å°è¦æ¨¡äº‹æ¥­è€…å‘ã‘ã«ITãƒ„ãƒ¼ãƒ«å°å…¥ã‚’æ”¯æ´ã€‚æ¥­å‹™åŠ¹ç‡åŒ–ã‚„å£²ä¸Šå‘ä¸Šã«è²¢çŒ®ã™ã‚‹ITãƒ„ãƒ¼ãƒ«å°å…¥è²»ç”¨ã®ä¸€éƒ¨ã‚’è£œåŠ©ã€‚",
                "deadline": "2025å¹´6æœˆé ƒï¼ˆè©³ç´°ã¯Webã‚µã‚¤ãƒˆã§è¦ç¢ºèªï¼‰",
                "amount": "5ä¸‡å††ï½450ä¸‡å††",
                "ratio": "1/2ï¼ˆæœ€ä½è³ƒé‡‘è¿‘å‚ã®äº‹æ¥­è€…ã¯2/3ï¼‰"
            })
    
    # é•·é‡çœŒã®åŸºæœ¬åŠ©æˆé‡‘æƒ…å ±ã‚‚Webã‹ã‚‰å–å¾—
    try:
        print("ğŸ” é•·é‡çœŒã®åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ä¸­...")
        
        # é•·é‡çœŒã®Webã‚µã‚¤ãƒˆã‹ã‚‰æƒ…å ±å–å¾—
        nagano_urls = [
            "https://www.pref.nagano.lg.jp/keieishien/corona/kouzou-tenkan.html",  # é•·é‡çœŒãƒ—ãƒ©ã‚¹è£œåŠ©é‡‘
            "https://www.pref.nagano.lg.jp/rodokoyo/seisanseisupport.html"  # è³ƒä¸Šã’ãƒ»ç”Ÿç”£æ€§å‘ä¸Šã‚µãƒãƒ¼ãƒˆè£œåŠ©é‡‘
        ]
        
        for url in nagano_urls:
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
                response = requests.get(url, headers=headers, timeout=30)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, "html.parser")
                    
                    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
                    title_elem = soup.select_one("h1") or soup.select_one("h2")
                    title = title_elem.text.strip() if title_elem else "é•·é‡çœŒè£œåŠ©é‡‘"
                    
                    # å†…å®¹ã‚’å–å¾—
                    content_elem = soup.select_one("#main-contents") or soup.select_one("#tmp_contents")
                    content_text = content_elem.text if content_elem else ""
                    
                    # ç· ã‚åˆ‡ã‚Šã‚’æ¤œç´¢
                    deadline_patterns = [
                        r'ç· åˆ‡.*?[ï¼š:]\s*(.*?[0-9]{4}å¹´[0-9]{1,2}æœˆ[0-9]{1,2}æ—¥)',
                        r'ç”³è¾¼æœŸé™.*?[ï¼š:]\s*(.*?[0-9]{4}å¹´[0-9]{1,2}æœˆ[0-9]{1,2}æ—¥)', 
                        r'å‹Ÿé›†æœŸé–“.*?[ï¼š:]\s*(.*?ã¾ã§)',
                        r'å—ä»˜æœŸé–“.*?[ï¼š:]\s*(.*?ã¾ã§)',
                        r'([0-9]{4}å¹´[0-9]{1,2}æœˆ[0-9]{1,2}æ—¥).*(ç· åˆ‡|ç· ã‚åˆ‡ã‚Š|ã€†åˆ‡)'
                    ]
                    
                    deadline = "è¦ç¢ºèª"
                    for pattern in deadline_patterns:
                        match = re.search(pattern, content_text)
                        if match:
                            deadline = match.group(1).strip()
                            break
                    
                    # è£œåŠ©é‡‘é¡ã‚’æ¤œç´¢
                    amount_patterns = [
                        r'è£œåŠ©é¡.*?[ï¼š:]\s*(.*?å††)',
                        r'åŠ©æˆé¡.*?[ï¼š:]\s*(.*?å††)',
                        r'è£œåŠ©é‡‘é¡.*?[ï¼š:]\s*(.*?å††)',
                        r'ä¸Šé™.*?([0-9,]+ä¸‡å††)'
                    ]
                    
                    amount = "è¦ç¢ºèª"
                    for pattern in amount_patterns:
                        match = re.search(pattern, content_text)
                        if match:
                            amount = match.group(1).strip()
                            break
                    
                    # è£œåŠ©ç‡ã‚’æ¤œç´¢
                    ratio_patterns = [
                        r'è£œåŠ©ç‡.*?[ï¼š:]\s*(.*?åˆ†ã®.*?)',
                        r'åŠ©æˆç‡.*?[ï¼š:]\s*(.*?åˆ†ã®.*?)',
                        r'([0-9]/[0-9]ä»¥å†…)',
                        r'([0-9]åˆ†ã®[0-9]ä»¥å†…)'
                    ]
                    
                    ratio = "è¦ç¢ºèª"
                    for pattern in ratio_patterns:
                        match = re.search(pattern, content_text)
                        if match:
                            ratio = match.group(1).strip()
                            break
                    
                    # èª¬æ˜æ–‡ã‚’ç”Ÿæˆ
                    description = content_text[:200].replace("\n", " ").strip() + "..." if content_text else "é•·é‡çœŒã®è£œåŠ©é‡‘åˆ¶åº¦"
                    
                    # åŠ©æˆé‡‘æƒ…å ±ã‚’è¿½åŠ 
                    nagano_grant = {
                        "title": title,
                        "url": url,
                        "date": datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                        "description": description,
                        "deadline": deadline,
                        "amount": amount,
                        "ratio": ratio
                    }
                    
                    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ•´å½¢ï¼ˆé•·ã™ãã‚‹å ´åˆï¼‰
                    if len(nagano_grant["title"]) > 50:
                        if "ãƒ—ãƒ©ã‚¹è£œåŠ©é‡‘" in nagano_grant["title"]:
                            nagano_grant["title"] = "é•·é‡çœŒãƒ—ãƒ©ã‚¹è£œåŠ©é‡‘ï¼ˆä¸­å°ä¼æ¥­çµŒå–¶æ§‹é€ è»¢æ›ä¿ƒé€²äº‹æ¥­ï¼‰"
                        elif "è³ƒä¸Šã’" in nagano_grant["title"] or "ç”Ÿç”£æ€§å‘ä¸Š" in nagano_grant["title"]:
                            nagano_grant["title"] = "é•·é‡çœŒä¸­å°ä¼æ¥­è³ƒä¸Šã’ãƒ»ç”Ÿç”£æ€§å‘ä¸Šã‚µãƒãƒ¼ãƒˆè£œåŠ©é‡‘"
                        else:
                            nagano_grant["title"] = nagano_grant["title"][:50] + "..."
                    
                    national_grants.append(nagano_grant)
            
            except Exception as e:
                print(f"âŒ é•·é‡çœŒåŠ©æˆé‡‘æƒ…å ±ã®å–å¾—ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
        
        print(f"âœ… é•·é‡çœŒåŠ©æˆé‡‘æƒ…å ±å–å¾—å®Œäº†: {len(national_grants)}ä»¶")
        
    except Exception as e:
        print(f"âŒ é•·é‡çœŒåŠ©æˆé‡‘æƒ…å ±ã®å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    return national_grants

# --- è¿½åŠ æƒ…å ±ã‚½ãƒ¼ã‚¹é–¢æ•° ---
def scrape_additional_sources():
    """è¿½åŠ ã®æƒ…å ±ã‚½ãƒ¼ã‚¹ã‹ã‚‰åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ã™ã‚‹ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
    additional_grants = []
    previous_grants = 0
    
    # ãƒŸãƒ©ã‚µãƒplusï¼ˆä¸­å°ä¼æ¥­åºã®ç·åˆæ”¯æ´ã‚µã‚¤ãƒˆï¼‰ã‹ã‚‰æƒ…å ±å–å¾—
    try:
        print("ğŸ” ãƒŸãƒ©ã‚µãƒplusã®æƒ…å ±ã‚’å–å¾—ä¸­...")
        mirasapo_url = "https://mirasapo-plus.go.jp/subsidy/"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(mirasapo_url, headers=headers, timeout=30)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            
            # è£œåŠ©é‡‘ãƒ»åŠ©æˆé‡‘ã®ä¸€è¦§ã‚’å–å¾—
            subsidy_items = soup.select(".subsidy-item") or soup.select(".list_subsidy li") or soup.select(".contents-list li")
            
            for item in subsidy_items:
                title_elem = item.select_one(".subsidy-item-title") or item.select_one("h3") or item.select_one("a") or item.select_one(".title")
                if not title_elem:
                    continue
                
                title = title_elem.text.strip()
                
                # URLã‚’å–å¾—
                link_elem = title_elem if title_elem.name == "a" else title_elem.find("a") or item.find("a")
                if not link_elem or not link_elem.get("href"):
                    continue
                    
                url = link_elem.get("href")
                if not url.startswith("http"):
                    url = urljoin("https://mirasapo-plus.go.jp", url)
                
                # è©³ç´°ãƒšãƒ¼ã‚¸ã®æƒ…å ±ã‚’å–å¾—ã—ã¦ã¿ã‚‹
                try:
                    details = scrape_grant_details(url)
                    description = details.get("description", "")
                    deadline = details.get("deadline", "")
                    amount = details.get("amount", "")
                    ratio = details.get("ratio", "")
                except:
                    # è©³ç´°ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ãŸå ´åˆã¯ã€ãƒªã‚¹ãƒˆå†…ã®æƒ…å ±ã ã‘ã§é€²ã‚ã‚‹
                    description = ""
                    deadline = ""
                    amount = ""
                    ratio = ""
                
                # ãƒªã‚¹ãƒˆå†…ã®æƒ…å ±ã‚’å–å¾—ï¼ˆè©³ç´°ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ãŸå ´åˆã®ä»£æ›¿æƒ…å ±ï¼‰
                if not description:
                    desc_elem = item.select_one(".subsidy-item-description") or item.select_one("p") or item.select_one(".text")
                    if desc_elem:
                        description = desc_elem.text.strip()
                
                # ç· ã‚åˆ‡ã‚Šã‚’å–å¾—
                if not deadline:
                    deadline_elem = item.select_one(".subsidy-item-deadline") or item.select_one(".date") or item.select_one(".period")
                    if deadline_elem:
                        deadline = deadline_elem.text.strip()
                
                # é‡‘é¡æƒ…å ±ã‚’å–å¾—
                if not amount:
                    amount_elem = item.select_one(".subsidy-item-amount") or item.select_one(".money") or item.select_one(".price")
                    if amount_elem:
                        amount = amount_elem.text.strip()
                
                # è©³ç´°æƒ…å ±ãŒå–å¾—ã§ããªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if not description:
                    description = f"{title}ã«é–¢ã™ã‚‹è£œåŠ©é‡‘ãƒ»åŠ©æˆé‡‘åˆ¶åº¦"
                if not deadline:
                    deadline = "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                if not amount:
                    amount = "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                if not ratio:
                    ratio = "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                
                # åŠ©æˆé‡‘æƒ…å ±ã‚’è¿½åŠ 
                additional_grants.append({
                    "title": title,
                    "url": url,
                    "date": datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                    "description": description,
                    "deadline": deadline,
                    "amount": amount,
                    "ratio": ratio
                })
            
            print(f"âœ… ãƒŸãƒ©ã‚µãƒplusã‹ã‚‰{len(additional_grants)}ä»¶ã®åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ ãƒŸãƒ©ã‚µãƒplusæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # çµŒæ¸ˆç”£æ¥­çœã®è£œåŠ©é‡‘ç·åˆã‚µã‚¤ãƒˆã‹ã‚‰æƒ…å ±å–å¾— - æ”¹å–„ç‰ˆ
    try:
        print("ğŸ” çµŒæ¸ˆç”£æ¥­çœã®è£œåŠ©é‡‘æƒ…å ±ã‚’å–å¾—ä¸­...")
        meti_urls = [
            "https://www.meti.go.jp/policy/hojyokin/index.html",
            "https://www.meti.go.jp/information/publicoffer/kobo.html"  # å…¬å‹Ÿæƒ…å ±ã®ãƒšãƒ¼ã‚¸ã‚‚è¿½åŠ 
        ]
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        previous_grants = len(additional_grants)
        
        for meti_url in meti_urls:
            try:
                response = requests.get(meti_url, headers=headers, timeout=30)
                if response.status_code == 200:
                    response.encoding = 'utf-8'  # çµŒç”£çœã‚µã‚¤ãƒˆã¯æ–‡å­—ã‚³ãƒ¼ãƒ‰æŒ‡å®šãŒå¿…è¦ãªå ´åˆãŒã‚ã‚‹
                    soup = BeautifulSoup(response.text, "html.parser")
                    
                    # è£œåŠ©é‡‘ãƒ»åŠ©æˆé‡‘ã®ä¸€è¦§ã‚’å–å¾—ï¼ˆçµŒç”£çœã‚µã‚¤ãƒˆã®æ§‹é€ ã«åˆã‚ã›ã¦èª¿æ•´ï¼‰
                    subsidy_links = soup.select("a[href*='hojyo']") or soup.select("a[href*='subsidy']") or soup.select("a[href*='kobo']") or soup.select(".subsidy") or soup.select(".news-list a")
                    
                    for link in subsidy_links:
                        title = link.text.strip()
                        if not title or len(title) < 5:  # çŸ­ã™ãã‚‹ã‚¿ã‚¤ãƒˆãƒ«ã¯é™¤å¤–
                            continue
                        
                        # åŠ©æˆé‡‘ãƒ»è£œåŠ©é‡‘ã«é–¢é€£ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹ã‚‚ã®ã ã‘ã‚’å¯¾è±¡ã«ã™ã‚‹
                        if not any(keyword in title.lower() for keyword in ['è£œåŠ©', 'åŠ©æˆ', 'æ”¯æ´', 'çµ¦ä»˜', 'äº¤ä»˜', 'å…¬å‹Ÿ', 'å‹Ÿé›†']):
                            continue
                        
                        url = link.get("href")
                        if not url:
                            continue
                            
                        if not url.startswith("http"):
                            url = urljoin("https://www.meti.go.jp", url)
                        
                        # è©³ç´°ãƒšãƒ¼ã‚¸ã®æƒ…å ±ã‚’å–å¾—ã—ã¦ã¿ã‚‹
                        try:
                            details = scrape_grant_details(url)
                            description = details.get("description", "")
                            deadline = details.get("deadline", "")
                            amount = details.get("amount", "")
                            ratio = details.get("ratio", "")
                        except:
                            # è©³ç´°ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ãŸå ´åˆ
                            description = "çµŒæ¸ˆç”£æ¥­çœã®åŠ©æˆé‡‘ãƒ»è£œåŠ©é‡‘åˆ¶åº¦"
                            deadline = "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                            amount = "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                            ratio = "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                        
                        # è©³ç´°æƒ…å ±ãŒå–å¾—ã§ããªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                        if not description:
                            description = "çµŒæ¸ˆç”£æ¥­çœã®åŠ©æˆé‡‘ãƒ»è£œåŠ©é‡‘åˆ¶åº¦"
                        if not deadline:
                            deadline = "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                        if not amount:
                            amount = "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                        if not ratio:
                            ratio = "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                        
                        # åŠ©æˆé‡‘æƒ…å ±ã‚’è¿½åŠ 
                        additional_grants.append({
                            "title": title,
                            "url": url,
                            "date": datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                            "description": description,
                            "deadline": deadline,
                            "amount": amount,
                            "ratio": ratio
                        })
            except Exception as e:
                print(f"âŒ çµŒæ¸ˆç”£æ¥­çœæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼ ({meti_url}): {e}")
        
        print(f"âœ… çµŒæ¸ˆç”£æ¥­çœã‹ã‚‰{len(additional_grants) - previous_grants}ä»¶ã®åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ çµŒæ¸ˆç”£æ¥­çœæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        
    # Gãƒ“ã‚ºIDãƒãƒ¼ã‚¿ãƒ«ã‹ã‚‰åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—
    try:
        print("ğŸ” Gãƒ“ã‚ºIDãƒãƒ¼ã‚¿ãƒ«ã®æƒ…å ±ã‚’å–å¾—ä¸­...")
        gbiz_url = "https://gbiz-id.go.jp/subsidies/"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(gbiz_url, headers=headers, timeout=30)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            previous_grants = len(additional_grants)
            
            # è£œåŠ©é‡‘ãƒ»åŠ©æˆé‡‘ã®ä¸€è¦§ã‚’å–å¾—
            subsidy_items = soup.select(".subsidy-item") or soup.select(".subsidy-list li") or soup.select("li.subsidy") or soup.select("div.subsidy")
            
            for item in subsidy_items:
                title_elem = item.select_one(".subsidy-title") or item.select_one("h3") or item.select_one("strong") or item.select_one("a")
                if not title_elem:
                    continue
                
                title = title_elem.text.strip()
                
                # URLã‚’å–å¾—
                link_elem = item.select_one("a") or (title_elem if title_elem.name == "a" else None)
                if not link_elem or not link_elem.get("href"):
                    continue
                    
                url = link_elem.get("href")
                if not url.startswith("http"):
                    url = urljoin("https://gbiz-id.go.jp", url)
                
                # è©³ç´°ãƒšãƒ¼ã‚¸ã®æƒ…å ±ã‚’å–å¾—ã—ã¦ã¿ã‚‹
                try:
                    details = scrape_grant_details(url)
                    description = details.get("description", "")
                    deadline = details.get("deadline", "")
                    amount = details.get("amount", "")
                    ratio = details.get("ratio", "")
                except:
                    # è©³ç´°ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ãŸå ´åˆ
                    description = ""
                    deadline = ""
                    amount = ""
                    ratio = ""
                
                # ãƒªã‚¹ãƒˆå†…ã®æƒ…å ±ã‚’å–å¾—
                if not description:
                    desc_elem = item.select_one(".subsidy-description") or item.select_one("p") or item.select_one(".description")
                    if desc_elem:
                        description = desc_elem.text.strip()
                
                # ç· ã‚åˆ‡ã‚Šã‚’å–å¾—
                if not deadline:
                    deadline_elem = item.select_one(".deadline") or item.select_one(".subsidy-deadline") or item.select_one(".date")
                    if deadline_elem:
                        deadline = deadline_elem.text.strip()
                
                # é‡‘é¡æƒ…å ±ã‚’å–å¾—
                if not amount:
                    amount_elem = item.select_one(".subsidy-amount") or item.select_one(".amount")
                    if amount_elem:
                        amount = amount_elem.text.strip()
                
                # è©³ç´°æƒ…å ±ãŒå–å¾—ã§ããªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if not description:
                    description = f"Gãƒ“ã‚ºIDå¯¾å¿œã®{title}ã«é–¢ã™ã‚‹åŠ©æˆé‡‘åˆ¶åº¦"
                if not deadline:
                    deadline = "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                if not amount:
                    amount = "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                if not ratio:
                    ratio = "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                
                # åŠ©æˆé‡‘æƒ…å ±ã‚’è¿½åŠ 
                additional_grants.append({
                    "title": title,
                    "url": url,
                    "date": datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                    "description": description,
                    "deadline": deadline,
                    "amount": amount,
                    "ratio": ratio
                })
            
            print(f"âœ… Gãƒ“ã‚ºIDãƒãƒ¼ã‚¿ãƒ«ã‹ã‚‰{len(additional_grants) - previous_grants}ä»¶ã®åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ Gãƒ“ã‚ºIDãƒãƒ¼ã‚¿ãƒ«æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # é•·é‡çœŒä¸­å°ä¼æ¥­æŒ¯èˆˆã‚»ãƒ³ã‚¿ãƒ¼ã®æƒ…å ±å–å¾—
    try:
        print("ğŸ” é•·é‡çœŒä¸­å°ä¼æ¥­æŒ¯èˆˆã‚»ãƒ³ã‚¿ãƒ¼ã®æƒ…å ±ã‚’å–å¾—ä¸­...")
        nagano_center_urls = [
            "https://www.nice-nagano.or.jp/topics/",
            "https://www.nice-nagano.or.jp/business/"  # ãƒ“ã‚¸ãƒã‚¹æ”¯æ´æƒ…å ±ã‚‚è¿½åŠ 
        ]
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        previous_grants = len(additional_grants)
        
        for nagano_center_url in nagano_center_urls:
            try:
                response = requests.get(nagano_center_url, headers=headers, timeout=30)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, "html.parser")
                    
                    # è£œåŠ©é‡‘ãƒ»åŠ©æˆé‡‘ã®ä¸€è¦§ã‚’å–å¾—ï¼ˆã‚µã‚¤ãƒˆæ§‹é€ ã«åˆã‚ã›ã¦èª¿æ•´ï¼‰
                    subsidy_items = soup.select(".topics-list li") or soup.select(".news-list li") or soup.select("article") or soup.select(".post")
                    
                    for item in subsidy_items:
                        # è£œåŠ©é‡‘ãƒ»åŠ©æˆé‡‘ã«é–¢é€£ã™ã‚‹é …ç›®ã®ã¿ã‚’æŠ½å‡º
                        item_text = item.text.lower()
                        if not any(keyword in item_text for keyword in ['è£œåŠ©', 'åŠ©æˆ', 'æ”¯æ´é‡‘', 'çµ¦ä»˜é‡‘', 'åŠ©é‡‘']):
                            continue
                        
                        title_elem = item.select_one("h3") or item.select_one("h4") or item.select_one("a") or item.select_one("strong") or item.select_one(".title")
                        if not title_elem:
                            continue
                        
                        title = title_elem.text.strip()
                        
                        # URLã‚’å–å¾—
                        link_elem = title_elem if title_elem.name == "a" else item.select_one("a")
                        if not link_elem or not link_elem.get("href"):
                            continue
                            
                        url = link_elem.get("href")
                        if not url.startswith("http"):
                            url = urljoin("https://www.nice-nagano.or.jp", url)
                        
                        # è©³ç´°æƒ…å ±ã‚’å–å¾—
                        description = item.text.strip()
                        if title in description:
                            description = description.replace(title, "").strip()
                        
                        # æ—¥ä»˜ã‚’æŠ½å‡º
                        date_elem = item.select_one(".date") or item.select_one("time") or item.select_one(".publish-date")
                        date_text = date_elem.text.strip() if date_elem else ""
                        
                        # ç· ã‚åˆ‡ã‚Šã‚’æŠ½å‡ºï¼ˆè©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—ã‚’è©¦ã¿ã‚‹ï¼‰
                        try:
                            details = scrape_grant_details(url)
                            detail_description = details.get("description", "")
                            if detail_description:
                                description = detail_description  # è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰ã®èª¬æ˜ã‚’ä½¿ç”¨
                            
                            deadline = details.get("deadline", "")
                            amount = details.get("amount", "")
                            ratio = details.get("ratio", "")
                        except:
                            # è©³ç´°ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ãŸå ´åˆ
                            deadline = ""
                            amount = ""
                            ratio = ""
                        
                        # ãƒªã‚¹ãƒˆå†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç· ã‚åˆ‡ã‚Šã‚’æ¢ã™ï¼ˆè©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—ã§ããªã‹ã£ãŸå ´åˆï¼‰
                        if not deadline:
                            deadline_match = re.search(r'([0-9]{4}å¹´[0-9]{1,2}æœˆ[0-9]{1,2}æ—¥).*(ç· åˆ‡|ç· ã‚åˆ‡ã‚Š|ã€†åˆ‡|ã¾ã§)', description)
                            if deadline_match:
                                deadline = deadline_match.group(1)
                            else:
                                deadline = "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                        
                        # è©³ç´°æƒ…å ±ãŒå–å¾—ã§ããªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                        if not amount:
                            amount = "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                        if not ratio:
                            ratio = "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                        
                        # åŠ©æˆé‡‘æƒ…å ±ã‚’è¿½åŠ 
                        additional_grants.append({
                            "title": title,
                            "url": url,
                            "date": date_text if date_text else datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                            "description": description[:200] + "..." if len(description) > 200 else description,
                            "deadline": deadline,
                            "amount": amount,
                            "ratio": ratio
                        })
            except Exception as e:
                print(f"âŒ é•·é‡çœŒä¸­å°ä¼æ¥­æŒ¯èˆˆã‚»ãƒ³ã‚¿ãƒ¼æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼ ({nagano_center_url}): {e}")
        
        print(f"âœ… é•·é‡çœŒä¸­å°ä¼æ¥­æŒ¯èˆˆã‚»ãƒ³ã‚¿ãƒ¼ã‹ã‚‰{len(additional_grants) - previous_grants}ä»¶ã®åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ é•·é‡çœŒä¸­å°ä¼æ¥­æŒ¯èˆˆã‚»ãƒ³ã‚¿ãƒ¼æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # æ—¥æœ¬å•†å·¥ä¼šè­°æ‰€ã®æƒ…å ±å–å¾—
    try:
        print("ğŸ” æ—¥æœ¬å•†å·¥ä¼šè­°æ‰€ã®åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ä¸­...")
        jcci_urls = [
            "https://www.jcci.or.jp/news/",
            "https://www.jcci.or.jp/sme/"  # ä¸­å°ä¼æ¥­æ”¯æ´æƒ…å ±ã‚‚è¿½åŠ 
        ]
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        previous_grants = len(additional_grants)
        
        for jcci_url in jcci_urls:
            try:
                response = requests.get(jcci_url, headers=headers, timeout=30)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, "html.parser")
                    
                    # ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ã‹ã‚‰è£œåŠ©é‡‘ãƒ»åŠ©æˆé‡‘é–¢é€£ã®æƒ…å ±ã‚’å–å¾—
                    news_items = soup.select(".news-list li") or soup.select(".news-item") or soup.select("article") or soup.select(".post")
                    
                    for item in news_items:
                        # è£œåŠ©é‡‘ãƒ»åŠ©æˆé‡‘ã«é–¢é€£ã™ã‚‹é …ç›®ã®ã¿ã‚’æŠ½å‡º
                        item_text = item.text.lower()
                        if not any(keyword in item_text for keyword in ['è£œåŠ©', 'åŠ©æˆ', 'æ”¯æ´é‡‘', 'çµ¦ä»˜é‡‘', 'å…¬å‹Ÿ']):
                            continue
                        
                        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
                        title_elem = item.select_one("h3") or item.select_one("h4") or item.select_one("a") or item.select_one(".title")
                        if not title_elem:
                            continue
                        
                        title = title_elem.text.strip()
                        
                        # URLã‚’å–å¾—
                        link_elem = title_elem if title_elem.name == "a" else item.select_one("a")
                        if not link_elem or not link_elem.get("href"):
                            continue
                            
                        url = link_elem.get("href")
                        if not url.startswith("http"):
                            url = urljoin("https://www.jcci.or.jp", url)
                        
                        # è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’å–å¾—
                        try:
                            details = scrape_grant_details(url)
                            description = details.get("description", "")
                            deadline = details.get("deadline", "")
                            amount = details.get("amount", "")
                            ratio = details.get("ratio", "")
                        except:
                            # è©³ç´°ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ãŸå ´åˆ
                            description = "æ—¥æœ¬å•†å·¥ä¼šè­°æ‰€ã‹ã‚‰ã®æƒ…å ±æä¾›"
                            deadline = "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                            amount = "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                            ratio = "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                        
                        # è©³ç´°æƒ…å ±ãŒå–å¾—ã§ããªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                        if not description:
                            description = "æ—¥æœ¬å•†å·¥ä¼šè­°æ‰€ã‹ã‚‰ã®æƒ…å ±æä¾›"
                        if not deadline:
                            deadline = "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                        if not amount:
                            amount = "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                        if not ratio:
                            ratio = "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                        
                        # æ—¥ä»˜ã‚’å–å¾—
                        date_elem = item.select_one(".date") or item.select_one("time")
                        date_text = date_elem.text.strip() if date_elem else datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
                        
                        # åŠ©æˆé‡‘æƒ…å ±ã‚’è¿½åŠ 
                        additional_grants.append({
                            "title": title,
                            "url": url,
                            "date": date_text,
                            "description": description,
                            "deadline": deadline,
                            "amount": amount,
                            "ratio": ratio
                        })
            except Exception as e:
                print(f"âŒ æ—¥æœ¬å•†å·¥ä¼šè­°æ‰€æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼ ({jcci_url}): {e}")
        
        print(f"âœ… æ—¥æœ¬å•†å·¥ä¼šè­°æ‰€ã‹ã‚‰{len(additional_grants) - previous_grants}ä»¶ã®åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ æ—¥æœ¬å•†å·¥ä¼šè­°æ‰€æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ã‚‚ã®ã¥ãã‚Šè£œåŠ©é‡‘å…¬å¼ã‚µã‚¤ãƒˆã®æƒ…å ±å–å¾—
    try:
        print("ğŸ” ã‚‚ã®ã¥ãã‚Šè£œåŠ©é‡‘ã®æƒ…å ±ã‚’å–å¾—ä¸­...")
        monodukuri_url = "https://portal.monodukuri-hojo.jp/"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(monodukuri_url, headers=headers, timeout=30)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            previous_grants = len(additional_grants)
            
            # å…¬å‹Ÿæƒ…å ±ã‚’å–å¾—
            info_blocks = soup.select(".info-block") or soup.select(".news-block") or soup.select("article")
            
            if not info_blocks:
                # å…¬å‹Ÿæƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæƒ…å ±ã‚’è¿½åŠ 
                additional_grants.append({
                    "title": "ã‚‚ã®ã¥ãã‚Šãƒ»å•†æ¥­ãƒ»ã‚µãƒ¼ãƒ“ã‚¹ç”Ÿç”£æ€§å‘ä¸Šä¿ƒé€²è£œåŠ©é‡‘",
                    "url": "https://portal.monodukuri-hojo.jp/",
                    "date": datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                    "description": "ä¸­å°ä¼æ¥­ãƒ»å°è¦æ¨¡äº‹æ¥­è€…ç­‰ãŒå–ã‚Šçµ„ã‚€é©æ–°çš„ã‚µãƒ¼ãƒ“ã‚¹é–‹ç™ºãƒ»è©¦ä½œå“é–‹ç™ºãƒ»ç”Ÿç”£ãƒ—ãƒ­ã‚»ã‚¹ã®æ”¹å–„ã‚’è¡Œã†ãŸã‚ã®è¨­å‚™æŠ•è³‡ç­‰ã‚’æ”¯æ´ã™ã‚‹è£œåŠ©é‡‘åˆ¶åº¦",
                    "deadline": "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª",
                    "amount": "æœ€å¤§1,000ä¸‡å††ï½2,000ä¸‡å††ï¼ˆé¡å‹ã«ã‚ˆã‚‹ï¼‰",
                    "ratio": "1/2ã€œ2/3ï¼ˆå°è¦æ¨¡äº‹æ¥­è€…ã¯2/3ï¼‰"
                })
            else:
                for block in info_blocks:
                    title_elem = block.select_one("h3") or block.select_one("h4") or block.select_one(".title")
                    if not title_elem:
                        continue
                    
                    title = title_elem.text.strip()
                    if "å…¬å‹Ÿ" in title or "å‹Ÿé›†" in title or "ç”³è«‹" in title:
                        # å…¬å‹Ÿã«é–¢ã™ã‚‹æƒ…å ±ã‚’æŠ½å‡º
                        description = block.text.strip()
                        if title in description:
                            description = description.replace(title, "").strip()
                        
                        # ãƒªãƒ³ã‚¯ã‚’å–å¾—
                        link_elem = block.select_one("a")
                        if link_elem and link_elem.get("href"):
                            url = link_elem.get("href")
                            if not url.startswith("http"):
                                url = urljoin("https://portal.monodukuri-hojo.jp/", url)
                        else:
                            url = "https://portal.monodukuri-hojo.jp/"
                        
                        # ç· ã‚åˆ‡ã‚Šã‚’æŠ½å‡º
                        deadline_match = re.search(r'([0-9]{4}å¹´[0-9]{1,2}æœˆ[0-9]{1,2}æ—¥).*(ç· åˆ‡|ç· ã‚åˆ‡ã‚Š|ã€†åˆ‡|ã¾ã§)', description)
                        deadline = deadline_match.group(1) if deadline_match else "è©³ç´°ã¯Webã‚µã‚¤ãƒˆã§ç¢ºèª"
                        
                        # åŠ©æˆé‡‘æƒ…å ±ã‚’è¿½åŠ 
                        additional_grants.append({
                            "title": "ã‚‚ã®ã¥ãã‚Šãƒ»å•†æ¥­ãƒ»ã‚µãƒ¼ãƒ“ã‚¹ç”Ÿç”£æ€§å‘ä¸Šä¿ƒé€²è£œåŠ©é‡‘ï¼ˆ" + title + "ï¼‰",
                            "url": url,
                            "date": datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
                            "description": description[:200] + "..." if len(description) > 200 else description,
                            "deadline": deadline,
                            "amount": "æœ€å¤§1,000ä¸‡å††ï½2,000ä¸‡å††ï¼ˆé¡å‹ã«ã‚ˆã‚‹ï¼‰",
                            "ratio": "1/2ã€œ2/3ï¼ˆå°è¦æ¨¡äº‹æ¥­è€…ã¯2/3ï¼‰"
                        })
            
            print(f"âœ… ã‚‚ã®ã¥ãã‚Šè£œåŠ©é‡‘ã‹ã‚‰{len(additional_grants) - previous_grants}ä»¶ã®åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ ã‚‚ã®ã¥ãã‚Šè£œåŠ©é‡‘æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # é‡è¤‡ã‚’æ’é™¤ã—ã¦è¿”ã™
    unique_grants = []
    urls = set()
    titles = set()
    
    for grant in additional_grants:
        # URLã¨ã‚¿ã‚¤ãƒˆãƒ«ã®ä¸¡æ–¹ãŒé‡è¤‡ã—ã¦ã„ãªã„å ´åˆã®ã¿è¿½åŠ 
        url_key = grant["url"].split("?")[0]  # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é™¤å¤–
        title_key = normalize_text(grant["title"])
        
        if url_key not in urls and title_key not in titles:
            urls.add(url_key)
            titles.add(title_key)
            unique_grants.append(grant)
    
    print(f"âœ… è¿½åŠ æƒ…å ±ã‚½ãƒ¼ã‚¹ã‹ã‚‰åˆè¨ˆ{len(unique_grants)}ä»¶ã®åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
    return unique_grants

# --- ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨GPTè©•ä¾¡é–¢æ•° ---
def filter_grants_for_target_business(grants, location="é•·é‡çœŒå¡©å°»å¸‚", industry="æƒ…å ±é€šä¿¡æ¥­", employees=56):
    """å¯¾è±¡ä¼æ¥­ã«é©ã—ãŸåŠ©æˆé‡‘æƒ…å ±ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
    # æƒ…å ±é€šä¿¡æ¥­å‘ã‘åŠ©æˆé‡‘ã«é–¢é€£ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    it_keywords = ['IT', 'ã‚·ã‚¹ãƒ†ãƒ ', 'ãƒ‡ã‚¸ã‚¿ãƒ«', 'æƒ…å ±é€šä¿¡', 'DX', 'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£', 'ã‚¢ãƒ—ãƒª', 'ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢', 
                  'ICT', 'ã‚¯ãƒ©ã‚¦ãƒ‰', 'AI', 'IoT', 'æŠ€è¡“', 'ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼', 'ã‚ªãƒ³ãƒ©ã‚¤ãƒ³', 'ãƒ‡ãƒ¼ã‚¿']
    
    # ä¸€æ¬¡ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆãƒ—ãƒ­ã‚°ãƒ©ãƒ çš„ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰
    filtered_grants = []
    
    for grant in grants:
        # åŸºæœ¬çš„ã«ã™ã¹ã¦ã®å…¨å›½å‘ã‘åŠ©æˆé‡‘ã¯å«ã‚ã‚‹
        include = True
        
        title = grant["title"].lower()
        desc = grant.get("description", "").lower()
        
        # ç‰¹å®šã®åœ°åŸŸé™å®šã§ã€ã‹ã¤å¯¾è±¡åœ°åŸŸã§ãªã„å ´åˆã¯é™¤å¤–
        if any(prefecture in title for prefecture in ['åŒ—æµ·é“', 'é’æ£®', 'å²©æ‰‹', 'å®®åŸ', 'ç§‹ç”°', 
                                               'å±±å½¢', 'ç¦å³¶', 'èŒ¨åŸ', 'æ ƒæœ¨', 'ç¾¤é¦¬', 
                                               'åŸ¼ç‰', 'åƒè‘‰', 'æ±äº¬', 'ç¥å¥ˆå·', 'æ–°æ½Ÿ', 
                                               'å¯Œå±±', 'çŸ³å·', 'ç¦äº•', 'å±±æ¢¨', 'å²é˜œ', 
                                               'é™å²¡', 'æ„›çŸ¥', 'ä¸‰é‡', 'æ»‹è³€', 'äº¬éƒ½', 
                                               'å¤§é˜ª', 'å…µåº«', 'å¥ˆè‰¯', 'å’Œæ­Œå±±', 'é³¥å–', 
                                               'å³¶æ ¹', 'å²¡å±±', 'åºƒå³¶', 'å±±å£', 'å¾³å³¶', 
                                               'é¦™å·', 'æ„›åª›', 'é«˜çŸ¥', 'ç¦å²¡', 'ä½è³€', 
                                               'é•·å´', 'ç†Šæœ¬', 'å¤§åˆ†', 'å®®å´', 'é¹¿å…å³¶', 
                                               'æ²–ç¸„']) and not any(keyword in title for keyword in ['é•·é‡', 'å…¨å›½', 'å…¨ã¦', 'ã™ã¹ã¦']):
            include = False
        
        # ç‰¹å®šã®æ¥­ç¨®é™å®šã§ã€æƒ…å ±é€šä¿¡æ¥­ãŒå¯¾è±¡å¤–ã®å ´åˆã¯é™¤å¤–
        # ä¾‹ï¼šè¾²æ¥­ã€æ¼æ¥­ã®ã¿å¯¾è±¡ã§ã€ã‹ã¤ITé–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ãªã„å ´åˆ
        if (('è¾²æ¥­' in title or 'è¾²æ—' in title or 'æ¼æ¥­' in title or 'æ—æ¥­' in title) and 
            not any(kw.lower() in title.lower() or kw.lower() in desc.lower() for kw in it_keywords)):
            include = False
        
        # æ˜ç¤ºçš„ã«é™¤å¤–ã•ã‚Œã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        exclude_keywords = ['çµ‚äº†ã—ã¾ã—ãŸ', 'å‹Ÿé›†çµ‚äº†', 'å—ä»˜çµ‚äº†', 'å‹Ÿé›†ã¯ç· ã‚åˆ‡ã‚Šã¾ã—ãŸ']
        if any(exclude_kw in title or exclude_kw in desc for exclude_kw in exclude_keywords):
            include = False
        
        # åœ°åŸŸã‚„æ¥­ç¨®ã«é–¢ã‚ã‚‰ãšã€ITç³»ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯å«ã‚ã‚‹
        if any(kw.lower() in title.lower() or kw.lower() in desc.lower() for kw in it_keywords):
            include = True
        
        if include:
            filtered_grants.append(grant)
    
    print(f"ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®åŠ©æˆé‡‘ä»¶æ•°: {len(filtered_grants)} ä»¶")
    return filtered_grants

def evaluate_grant_with_gpt(title, url, description, deadline, amount, ratio):
    """åŠ©æˆé‡‘æƒ…å ±ã‚’GPTã§è©•ä¾¡"""
    prompt = f"""
ã‚ãªãŸã¯ä¼æ¥­å‘ã‘åŠ©æˆé‡‘ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®åŠ©æˆé‡‘ãŒã€é•·é‡çœŒå¡©å°»å¸‚ã®æƒ…å ±é€šä¿¡æ¥­ãƒ»å¾“æ¥­å“¡56åã®ä¸­å°ä¼æ¥­ã«ã¨ã£ã¦ç”³è«‹å¯¾è±¡ã«ãªã‚‹ã‹ã€ã¾ãŸç”³è«‹å„ªå…ˆåº¦ï¼ˆé«˜ãƒ»ä¸­ãƒ»ä½ï¼‰ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

ã€åŠ©æˆé‡‘åã€‘{title}
ã€è©³ç´°URLã€‘{url}
ã€æ¦‚è¦ã€‘{description}
ã€ç”³è«‹æœŸé™ã€‘{deadline}
ã€åŠ©æˆé‡‘é¡ã€‘{amount}
ã€è£œåŠ©å‰²åˆã€‘{ratio}

å›ç­”å½¢å¼ã¯ä»¥ä¸‹ã§ãŠé¡˜ã„ã—ã¾ã™ï¼š
---
å¯¾è±¡ã‹ã©ã†ã‹: ï¼ˆã¯ã„ï¼ã„ã„ãˆï¼‰
ç†ç”±: ï¼ˆç°¡å˜ã«ï¼‰
ç”³è«‹å„ªå…ˆåº¦: ï¼ˆé«˜ï¼ä¸­ï¼ä½ï¼‰
---
"""
    try:
        response = openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"âŒ GPTè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {str(e)}"

# --- Google Chaté€šçŸ¥é–¢æ•° ---
def send_to_google_chat(message, webhook_url):
    """Google Chatã«é€šçŸ¥ã‚’é€ä¿¡"""
    if not message.strip():
        print("âŒ é€ä¿¡ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒç©ºã§ã™")
        message = "åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚·ã‚¹ãƒ†ãƒ ç®¡ç†è€…ã«ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
    
    # URLã®æ¤œè¨¼
    if not webhook_url or not webhook_url.startswith("https://"):
        print("âŒ ç„¡åŠ¹ãªwebhook URLã§ã™")
        return
    
    headers = {"Content-Type": "application/json; charset=UTF-8"}
    
    current_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M')
    
    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’çŸ­ãã¾ã¨ã‚ã‚‹
    summarized_message = f"ğŸ“¢ åŠ©æˆé‡‘æ”¯æ´åˆ¶åº¦è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ\næ›´æ–°æ—¥æ™‚: {current_time}\n\n"
    
    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡Œã”ã¨ã«åˆ†å‰²
    lines = message.strip().split('\n\n')
    grant_count = 0
    
    for i, grant_block in enumerate(lines):
        if not grant_block.strip():
            continue
            
        grant_count += 1
        # åŠ©æˆé‡‘ãƒ–ãƒ­ãƒƒã‚¯ã®è¡Œã‚’åˆ†å‰²
        block_lines = grant_block.split('\n')
        if len(block_lines) < 2:
            continue
        
        # ã‚¿ã‚¤ãƒˆãƒ«è¡Œã‚’æŠ½å‡ºãƒ»å‡¦ç†ï¼ˆæœ€åˆã®è¡Œï¼‰
        title_line = block_lines[0].replace('*', '')
        # ã‚¿ã‚¤ãƒˆãƒ«ã®ç•ªå·ã‚’å–å¾—
        title_num = re.search(r'^([0-9]+)\.', title_line)
        title_num = title_num.group(1) if title_num else str(grant_count)
        
        # ã‚¿ã‚¤ãƒˆãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆç•ªå·ã®å¾Œã®éƒ¨åˆ†ï¼‰
        title_text = re.sub(r'^[0-9]+\.\s*', '', title_line).strip()
        
        # æ–‡å­—åŒ–ã‘ã—ã¦ã„ã‚‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’ä¿®æ­£
        safe_title = generate_simple_title(title_text, title_num)
        
        # å¯¾è±¡ã¨å„ªå…ˆåº¦è¡Œã‚’æŠ½å‡ºï¼ˆé€šå¸¸2è¡Œç›®ã¨3è¡Œç›®ï¼‰
        target_line = next((line for line in block_lines if 'ãƒ»å¯¾è±¡:' in line), "ãƒ»å¯¾è±¡: ä¸æ˜")
        priority_line = next((line for line in block_lines if 'ãƒ»å„ªå…ˆåº¦:' in line), "ãƒ»å„ªå…ˆåº¦: ä¸æ˜")
        deadline_line = next((line for line in block_lines if 'ãƒ»ç”³è«‹æœŸé™:' in line), "ãƒ»ç”³è«‹æœŸé™: è¦ç¢ºèª")
        amount_line = next((line for line in block_lines if 'ãƒ»åŠ©æˆé‡‘é¡:' in line), "ãƒ»åŠ©æˆé‡‘é¡: è¦ç¢ºèª")
        ratio_line = next((line for line in block_lines if 'ãƒ»è£œåŠ©å‰²åˆ:' in line), "ãƒ»è£œåŠ©å‰²åˆ: è¦ç¢ºèª")
        
        # URLè¡Œã‚’æŠ½å‡ºï¼ˆé€šå¸¸æœ€å¾Œã®è¡Œï¼‰
        url_line = next((line for line in block_lines if 'ãƒ»URL:' in line), "")
        
        # ç°¡æ½”ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«æ•´å½¢
        summarized_message += f"{title_num}. {safe_title}\n{target_line}\n{priority_line}\n{deadline_line}\n{amount_line}\n{ratio_line}\n{url_line}\n\n"
    
    # ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã®ä½œæˆ
    payload = {"text": summarized_message}
    
    try:
        print(f"â³ Google Chatã«é€ä¿¡ä¸­... ({webhook_url[:15]}...)")
        encoded_payload = json.dumps(payload, ensure_ascii=False).encode('utf-8')
        response = requests.post(
            webhook_url, 
            headers=headers, 
            data=encoded_payload
        )
        print(f"å¿œç­”ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
        print(f"å¿œç­”æœ¬æ–‡: {response.text[:100]}")  # æœ€åˆã®100æ–‡å­—ã ã‘è¡¨ç¤º
        
        if response.status_code == 200:
            print(f"âœ… Google Chatã«é€šçŸ¥ã—ã¾ã—ãŸã€‚ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
        else:
            print(f"âŒ Google Chaté€ä¿¡ã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}")
            print(f"å¿œç­”æœ¬æ–‡: {response.text}")
    except Exception as e:
        print(f"âŒ Google Chaté€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆå†…å®¹: {encoded_payload[:200].decode('utf-8')}...")

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
def main():
    print("âœ… åŠ©æˆé‡‘æƒ…å ±å–å¾—é–‹å§‹")
    
    # J-Net21ã‹ã‚‰åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—
    grants = scrape_jnet21_grants()
    print(f"âœ… J-Net21ã‹ã‚‰åŠ©æˆé‡‘æƒ…å ±å–å¾—: {len(grants)} ä»¶")
    
    # è¿½åŠ æƒ…å ±ã‚½ãƒ¼ã‚¹ã‹ã‚‰åŠ©æˆé‡‘æƒ…å ±ã‚’å–å¾—ï¼ˆæ”¹å–„ç‰ˆé–¢æ•°ã‚’ä½¿ç”¨ï¼‰
    additional_grants = scrape_additional_sources()
    print(f"âœ… è¿½åŠ æƒ…å ±ã‚½ãƒ¼ã‚¹ã‹ã‚‰åŠ©æˆé‡‘æƒ…å ±å–å¾—: {len(additional_grants)} ä»¶")
    
    # åŠ©æˆé‡‘æƒ…å ±ã‚’ãƒãƒ¼ã‚¸
    grants.extend(additional_grants)
    
    # URLãƒ™ãƒ¼ã‚¹ã§é‡è¤‡ã‚’æ’é™¤
    unique_grants = []
    urls = set()
    titles = set()
    
    for grant in grants:
        # URLã¨ã‚¿ã‚¤ãƒˆãƒ«ã®ä¸¡æ–¹ãŒé‡è¤‡ã—ã¦ã„ãªã„å ´åˆã®ã¿è¿½åŠ 
        url_key = grant["url"].split("?")[0]  # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é™¤å¤–
        title_key = normalize_text(grant["title"])
        
        if url_key not in urls and title_key not in titles:
            urls.add(url_key)
            titles.add(title_key)
            unique_grants.append(grant)
    
    grants = unique_grants
    print(f"âœ… é‡è¤‡æ’é™¤å¾Œã®åŠ©æˆé‡‘ä»¶æ•°: {len(grants)} ä»¶")
    
    # å¯¾è±¡ä¼æ¥­å‘ã‘ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆæ”¹å–„ç‰ˆé–¢æ•°ã‚’ä½¿ç”¨ï¼‰
    grants = filter_grants_for_target_business(grants)
    
    # å–å¾—ã§ããŸä»¶æ•°ãŒå°‘ãªã„å ´åˆã¯ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
    if len(grants) < 3:
        print("âš ï¸ å–å¾—ã§ããŸåŠ©æˆé‡‘æƒ…å ±ãŒå°‘ãªã„ãŸã‚ã€ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨")
        backup_grants = get_national_grants()
        grants = backup_grants
    
    print(f"âœ… æœ€çµ‚åŠ©æˆé‡‘ä»¶æ•°: {len(grants)} ä»¶")

    # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆåˆæœŸåŒ–
    try:
        sheet.clear()
        headers = ["No.", "ã‚¿ã‚¤ãƒˆãƒ«", "URL", "ç”³è«‹æœŸé™", "åŠ©æˆé‡‘é¡", "è£œåŠ©å‰²åˆ", "å¯¾è±¡ã‹ã©ã†ã‹", "ç†ç”±", "ç”³è«‹å„ªå…ˆåº¦"]
        sheet.append_row(headers)
        print("âœ… ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆåˆæœŸåŒ–å®Œäº†")
    except Exception as e:
        print(f"âŒ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ“ä½œã‚¨ãƒ©ãƒ¼: {e}")
        # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡ã—ã¦çµ‚äº†
        send_to_google_chat("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®æ“ä½œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚", WEBHOOK_URL)
        return

    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å†…å®¹ã‚’åˆæœŸåŒ–
    full_message = ""

    for i, grant in enumerate(grants, start=1):
        # ã‚¿ã‚¤ãƒˆãƒ«ã®æ–‡å­—åŒ–ã‘ãƒã‚§ãƒƒã‚¯ã¨ä¿®æ­£
        title = normalize_text(grant["title"])
        # å¿…è¦ã«å¿œã˜ã¦ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¿ã‚¤ãƒˆãƒ«ã«ç½®ãæ›ãˆ
        title = generate_simple_title(title, i)
        
        url = grant["url"]
        description = normalize_text(grant.get("description", ""))
        deadline = normalize_text(grant.get("deadline", "è¦ç¢ºèª"))
        amount = normalize_text(grant.get("amount", "è¦ç¢ºèª"))
        ratio = normalize_text(grant.get("ratio", "è¦ç¢ºèª"))

        print(f"â³ {i}ä»¶ç›® è©•ä¾¡ä¸­...")
        result = evaluate_grant_with_gpt(title, url, description, deadline, amount, ratio)
        print(f"âœ… {i}ä»¶ç›® è©•ä¾¡å®Œäº†")

        # GPTå›ç­”ã®åˆ†è§£ï¼ˆæ­£è¦è¡¨ç¾ã‚’ä½¿ã£ã¦å …ç‰¢ã«ï¼‰
        target = re.search(r"å¯¾è±¡ã‹ã©ã†ã‹:?\s*(.+)", result)
        target = normalize_text(target.group(1).strip() if target else "ä¸æ˜")
        
        reason = re.search(r"ç†ç”±:?\s*(.+)", result)
        reason = normalize_text(reason.group(1).strip() if reason else "ä¸æ˜")
        
        priority = re.search(r"ç”³è«‹å„ªå…ˆåº¦:?\s*(.+)", result)
        priority = normalize_text(priority.group(1).strip() if priority else "ä¸æ˜")

        try:
            sheet.append_row([i, title, url, deadline, amount, ratio, target, reason, priority])
            print(f"âœ… {i}ä»¶ç›® ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ›¸ãè¾¼ã¿å®Œäº†")
        except Exception as e:
            print(f"âŒ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

        # å„åŠ©æˆé‡‘æƒ…å ±ã‚’ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«è¿½åŠ 
        full_message += f"*{i}. {title}*\n"
        full_message += f"ãƒ»å¯¾è±¡: *{target}*\n"
        full_message += f"ãƒ»å„ªå…ˆåº¦: *{priority}*\n"
        full_message += f"ãƒ»ç”³è«‹æœŸé™: {deadline}\n"
        full_message += f"ãƒ»åŠ©æˆé‡‘é¡: {amount}\n"  # åŠ©æˆé‡‘é¡ã‚‚è¡¨ç¤º
        full_message += f"ãƒ»è£œåŠ©å‰²åˆ: {ratio}\n"   # è£œåŠ©å‰²åˆã‚‚è¡¨ç¤º
        # é•·ã™ãã‚‹èª¬æ˜æ–‡ã¯ç°¡æ½”ã«ã™ã‚‹
        short_reason = reason[:100] + "..." if len(reason) > 100 else reason
        full_message += f"ãƒ»ç†ç”±: {short_reason}\n"
        full_message += f"ãƒ»URL: {url}\n\n"

    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒç©ºã§ãªã„ã“ã¨ã‚’ç¢ºèªã—ã¦ã‹ã‚‰é€ä¿¡
    if full_message:
        send_to_google_chat(full_message, WEBHOOK_URL)
    else:
        print("âŒ é€ä¿¡ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚ã‚Šã¾ã›ã‚“")
        send_to_google_chat("åŠ©æˆé‡‘æƒ…å ±ã®è©•ä¾¡çµæœã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", WEBHOOK_URL)

if __name__ == "__main__":
    main()
